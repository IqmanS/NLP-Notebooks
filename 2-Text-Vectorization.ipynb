{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b599d4",
   "metadata": {
    "papermill": {
     "duration": 0.007498,
     "end_time": "2023-07-02T06:07:31.097588",
     "exception": false,
     "start_time": "2023-07-02T06:07:31.090090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Getting Started with NLP**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8032b505",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-02T06:07:31.113972Z",
     "iopub.status.busy": "2023-07-02T06:07:31.113106Z",
     "iopub.status.idle": "2023-07-02T06:07:44.412050Z",
     "shell.execute_reply": "2023-07-02T06:07:44.410827Z"
    },
    "papermill": {
     "duration": 13.310466,
     "end_time": "2023-07-02T06:07:44.414910",
     "exception": false,
     "start_time": "2023-07-02T06:07:31.104444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /kaggle/working/...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /kaggle/working/...\n",
      "[nltk_data] Downloading package stopwords to /kaggle/working/...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import warnings\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "import sklearn\n",
    "nltk.download('punkt',download_dir=\"/kaggle/working/\")\n",
    "nltk.download('wordnet',download_dir=\"/kaggle/working/\")\n",
    "nltk.download('stopwords',download_dir=\"/kaggle/working/\")\n",
    "nltk.data.path.append('/kaggle/working/') \n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"/kaggle/working/corpora/wordnet.zip\", 'r') as zip_f:\n",
    "    zip_f.extractall(\"/kaggle/working/corpora/\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.style.use('dark_background')\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed6b9e56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T06:07:44.431830Z",
     "iopub.status.busy": "2023-07-02T06:07:44.430478Z",
     "iopub.status.idle": "2023-07-02T06:07:44.438609Z",
     "shell.execute_reply": "2023-07-02T06:07:44.437431Z"
    },
    "papermill": {
     "duration": 0.019357,
     "end_time": "2023-07-02T06:07:44.441345",
     "exception": false,
     "start_time": "2023-07-02T06:07:44.421988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stephen Hawking: “Questioning the Universe* Speech/Corpus\n",
    "speech = \"\"\"There is nothing bigger or older than the universe. \n",
    "The questions I would like to talk about are: one, where did we come from?\n",
    "How did the universe come into being? Are we alone in the universe?\n",
    "Is there alien life out there? What is the future of the human race?\n",
    "Up until the 1920s, everyone thought the universe was essentially static and unchanging in time.\n",
    "Then it was discovered that the universe was expanding. Distant galaxies were moving away from us.\n",
    "This meant they must have been closer together in the past. If we extrapolate back, \n",
    "we find we must have all been on top of each other about 15 billion years ago. \n",
    "This was the Big Bang, the beginning of the universe. But was there anything before the Big Bang?\n",
    "If not, what created the universe? Why did the universe emerge from the Big Bang the way it did?\n",
    "We used to think that the theory of the universe could be divided into two parts. \n",
    "First, there were the laws like Maxwell’s equations and general relativity that determined the\n",
    "evolution of the universe, given its state over all of the space at one time. And second, \n",
    "there was no question of the initial state of the universe. We have made good progress on the\n",
    "first part, and now have the knowledge of the laws of evolution in all but the most extreme conditions.\n",
    "But until recently, we have had little idea about the initial conditions for the universe.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb7ec1",
   "metadata": {
    "papermill": {
     "duration": 0.006752,
     "end_time": "2023-07-02T06:07:44.455246",
     "exception": false,
     "start_time": "2023-07-02T06:07:44.448494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# 1. Tokenization\n",
    "### 1.1 Sentence Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5386ebd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T06:07:44.471117Z",
     "iopub.status.busy": "2023-07-02T06:07:44.470649Z",
     "iopub.status.idle": "2023-07-02T06:07:44.495448Z",
     "shell.execute_reply": "2023-07-02T06:07:44.494356Z"
    },
    "papermill": {
     "duration": 0.035664,
     "end_time": "2023-07-02T06:07:44.498038",
     "exception": false,
     "start_time": "2023-07-02T06:07:44.462374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is nothing bigger or older than the universe.',\n",
       " 'The questions I would like to talk about are: one, where did we come from?',\n",
       " 'How did the universe come into being?',\n",
       " 'Are we alone in the universe?',\n",
       " 'Is there alien life out there?',\n",
       " 'What is the future of the human race?',\n",
       " 'Up until the 1920s, everyone thought the universe was essentially static and unchanging in time.',\n",
       " 'Then it was discovered that the universe was expanding.',\n",
       " 'Distant galaxies were moving away from us.',\n",
       " 'This meant they must have been closer together in the past.',\n",
       " 'If we extrapolate back, \\nwe find we must have all been on top of each other about 15 billion years ago.',\n",
       " 'This was the Big Bang, the beginning of the universe.',\n",
       " 'But was there anything before the Big Bang?',\n",
       " 'If not, what created the universe?',\n",
       " 'Why did the universe emerge from the Big Bang the way it did?',\n",
       " 'We used to think that the theory of the universe could be divided into two parts.',\n",
       " 'First, there were the laws like Maxwell’s equations and general relativity that determined the\\nevolution of the universe, given its state over all of the space at one time.',\n",
       " 'And second, \\nthere was no question of the initial state of the universe.',\n",
       " 'We have made good progress on the\\nfirst part, and now have the knowledge of the laws of evolution in all but the most extreme conditions.',\n",
       " 'But until recently, we have had little idea about the initial conditions for the universe.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(speech)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df7b834",
   "metadata": {
    "papermill": {
     "duration": 0.006883,
     "end_time": "2023-07-02T06:07:44.513865",
     "exception": false,
     "start_time": "2023-07-02T06:07:44.506982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.2 Word Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc294ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T06:07:44.530363Z",
     "iopub.status.busy": "2023-07-02T06:07:44.529558Z",
     "iopub.status.idle": "2023-07-02T06:07:44.541684Z",
     "shell.execute_reply": "2023-07-02T06:07:44.540697Z"
    },
    "papermill": {
     "duration": 0.023125,
     "end_time": "2023-07-02T06:07:44.544034",
     "exception": false,
     "start_time": "2023-07-02T06:07:44.520909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(speech)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f2bda64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T06:07:44.560693Z",
     "iopub.status.busy": "2023-07-02T06:07:44.559853Z",
     "iopub.status.idle": "2023-07-02T06:07:44.567173Z",
     "shell.execute_reply": "2023-07-02T06:07:44.565889Z"
    },
    "papermill": {
     "duration": 0.018324,
     "end_time": "2023-07-02T06:07:44.569561",
     "exception": false,
     "start_time": "2023-07-02T06:07:44.551237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? / How / did / the / universe / come / into / being / ? / Are / we / alone / in / the / universe / ? / Is / there / alien / life / out / there / ? / What / is / the / future / of / the / human / race / ? / Up / until / the / 1920s / , / everyone / thought / the / universe / was / essentially / static / and / unchanging / in / time / . / Then / it / was / discovered / that / the / universe / "
     ]
    }
   ],
   "source": [
    "for i in range(random.randint(1,50),random.randint(50,100)):\n",
    "    print(words[i],end=\" / \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87902ae7",
   "metadata": {
    "papermill": {
     "duration": 0.007008,
     "end_time": "2023-07-02T06:07:44.583801",
     "exception": false,
     "start_time": "2023-07-02T06:07:44.576793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# 2. Stemming vs Lemmatization\n",
    "\n",
    "### 2.1 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c78faad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T06:07:44.600151Z",
     "iopub.status.busy": "2023-07-02T06:07:44.599775Z",
     "iopub.status.idle": "2023-07-02T06:07:44.611298Z",
     "shell.execute_reply": "2023-07-02T06:07:44.609958Z"
    },
    "papermill": {
     "duration": 0.022533,
     "end_time": "2023-07-02T06:07:44.613517",
     "exception": false,
     "start_time": "2023-07-02T06:07:44.590984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab97eb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T06:07:44.631120Z",
     "iopub.status.busy": "2023-07-02T06:07:44.629820Z",
     "iopub.status.idle": "2023-07-02T06:07:44.650115Z",
     "shell.execute_reply": "2023-07-02T06:07:44.648736Z"
    },
    "papermill": {
     "duration": 0.031555,
     "end_time": "2023-07-02T06:07:44.652634",
     "exception": false,
     "start_time": "2023-07-02T06:07:44.621079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noth bigger older univers \n",
      "question would like talk  one  come \n",
      "univers come \n",
      "alon univers \n",
      "alien life \n",
      "futur human race \n",
      "1920  everyon thought univers essenti static unchang time \n",
      "discov univers expand \n",
      "distant galaxi move away us \n",
      "meant must closer togeth past \n",
      "extrapol back  find must top 15 billion year ago \n",
      "big bang  begin univers \n",
      "anyth big bang \n",
      " creat univers \n",
      "univers emerg big bang way \n",
      "use think theori univers could divid two part \n",
      "first  law like maxwel  equat gener rel determin evolut univers  given state space one time \n",
      "second  question initi state univers \n",
      "made good progress first part  knowledg law evolut extrem condit \n",
      "recent  littl idea initi condit univers \n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "stemmedSentences = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "#     words = [re.sub('[!,*)@#%(&$_?.^]',\"\",i).lower() for i in words]\n",
    "    words = [re.sub(\"[^a-zA-Z0-9]\",\"\",i).lower() for i in words]\n",
    "    words = [stemmer.stem(i) for i in words if i not in stopwords]\n",
    "    stemmedSentences.append(\" \".join(words))\n",
    "    print(stemmedSentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87317a",
   "metadata": {
    "papermill": {
     "duration": 0.007377,
     "end_time": "2023-07-02T06:07:44.667594",
     "exception": false,
     "start_time": "2023-07-02T06:07:44.660217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb4209c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T06:07:44.684500Z",
     "iopub.status.busy": "2023-07-02T06:07:44.684021Z",
     "iopub.status.idle": "2023-07-02T06:07:47.251297Z",
     "shell.execute_reply": "2023-07-02T06:07:47.250080Z"
    },
    "papermill": {
     "duration": 2.578531,
     "end_time": "2023-07-02T06:07:47.253726",
     "exception": false,
     "start_time": "2023-07-02T06:07:44.675195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing bigger older universe \n",
      "question would like talk  one  come \n",
      "universe come \n",
      "alone universe \n",
      "alien life \n",
      "future human race \n",
      "1920s  everyone thought universe essentially static unchanging time \n",
      "discovered universe expanding \n",
      "distant galaxy moving away u \n",
      "meant must closer together past \n",
      "extrapolate back  find must top 15 billion year ago \n",
      "big bang  beginning universe \n",
      "anything big bang \n",
      " created universe \n",
      "universe emerge big bang way \n",
      "used think theory universe could divided two part \n",
      "first  law like maxwell  equation general relativity determined evolution universe  given state space one time \n",
      "second  question initial state universe \n",
      "made good progress first part  knowledge law evolution extreme condition \n",
      "recently  little idea initial condition universe \n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemmatizedSentences = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "#     words = [re.sub(\"[!,*)@#%(&$_?.:’'^]\",\"\",i).lower() for i in words]\n",
    "    words = [re.sub(\"[^a-zA-Z0-9]\",\"\",i).lower() for i in words]\n",
    "    words = [lemmatizer.lemmatize(i) for i in words if i not in stopwords]\n",
    "    lemmatizedSentences.append(\" \".join(words))\n",
    "    print(lemmatizedSentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbefabe",
   "metadata": {
    "papermill": {
     "duration": 0.007336,
     "end_time": "2023-07-02T06:07:47.268662",
     "exception": false,
     "start_time": "2023-07-02T06:07:47.261326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.3 Comparing Stemming vs Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e271b03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T06:07:47.286614Z",
     "iopub.status.busy": "2023-07-02T06:07:47.285574Z",
     "iopub.status.idle": "2023-07-02T06:07:47.292367Z",
     "shell.execute_reply": "2023-07-02T06:07:47.290780Z"
    },
    "papermill": {
     "duration": 0.018893,
     "end_time": "2023-07-02T06:07:47.295271",
     "exception": false,
     "start_time": "2023-07-02T06:07:47.276378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920  everyon thought univers essenti static unchang time \n",
      "1920s  everyone thought universe essentially static unchanging time \n"
     ]
    }
   ],
   "source": [
    "print(stemmedSentences[6])\n",
    "print(lemmatizedSentences[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc57bc",
   "metadata": {
    "papermill": {
     "duration": 0.00765,
     "end_time": "2023-07-02T06:07:47.311089",
     "exception": false,
     "start_time": "2023-07-02T06:07:47.303439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# 3. Vectorization\n",
    "### 3.1 Bag of Words (CountVectorizer)\n",
    "- sklearn.feature_extraction.text.CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb7d0e79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T06:07:47.329000Z",
     "iopub.status.busy": "2023-07-02T06:07:47.328430Z",
     "iopub.status.idle": "2023-07-02T06:07:47.349572Z",
     "shell.execute_reply": "2023-07-02T06:07:47.348477Z"
    },
    "papermill": {
     "duration": 0.033302,
     "end_time": "2023-07-02T06:07:47.352113",
     "exception": false,
     "start_time": "2023-07-02T06:07:47.318811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 78)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequncy BoW\n",
    "countVectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features=2000)\n",
    "X = countVectorizer.fit_transform(lemmatizedSentences).toarray() \n",
    "X.shape\n",
    "# 20 - no of sentences\n",
    "# 78 - no of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c61b80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T06:07:47.370523Z",
     "iopub.status.busy": "2023-07-02T06:07:47.369723Z",
     "iopub.status.idle": "2023-07-02T06:07:47.376226Z",
     "shell.execute_reply": "2023-07-02T06:07:47.375023Z"
    },
    "papermill": {
     "duration": 0.018399,
     "end_time": "2023-07-02T06:07:47.378570",
     "exception": false,
     "start_time": "2023-07-02T06:07:47.360171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12007c5",
   "metadata": {
    "papermill": {
     "duration": 0.007972,
     "end_time": "2023-07-02T06:07:47.394568",
     "exception": false,
     "start_time": "2023-07-02T06:07:47.386596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.2 TF-IDF (Term Frequency * Inverse Document Frequency)\n",
    " - sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51a09cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T06:07:47.412989Z",
     "iopub.status.busy": "2023-07-02T06:07:47.412242Z",
     "iopub.status.idle": "2023-07-02T06:07:47.432365Z",
     "shell.execute_reply": "2023-07-02T06:07:47.431137Z"
    },
    "papermill": {
     "duration": 0.032382,
     "end_time": "2023-07-02T06:07:47.435077",
     "exception": false,
     "start_time": "2023-07-02T06:07:47.402695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 78)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfVecorier = sklearn.feature_extraction.text.TfidfVectorizer(max_features=2000)\n",
    "X = tfidfVecorier.fit_transform(lemmatizedSentences).toarray() \n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74331e87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T06:07:47.454080Z",
     "iopub.status.busy": "2023-07-02T06:07:47.453246Z",
     "iopub.status.idle": "2023-07-02T06:07:47.460112Z",
     "shell.execute_reply": "2023-07-02T06:07:47.458852Z"
    },
    "papermill": {
     "duration": 0.019055,
     "end_time": "2023-07-02T06:07:47.462380",
     "exception": false,
     "start_time": "2023-07-02T06:07:47.443325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.44321297 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.754646,
   "end_time": "2023-07-02T06:07:50.192010",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-02T06:07:18.437364",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
