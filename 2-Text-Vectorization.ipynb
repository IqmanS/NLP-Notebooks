{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/iqmansingh/getting-started-with-nlp?scriptVersionId=135478047\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **Getting Started with NLP**\n\n---","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport tensorflow as tf\nimport datetime\nimport warnings\nimport nltk\nimport random\nimport re\nimport sklearn\nnltk.download('punkt',download_dir=\"/kaggle/working/\")\nnltk.download('wordnet',download_dir=\"/kaggle/working/\")\nnltk.download('stopwords',download_dir=\"/kaggle/working/\")\nnltk.data.path.append('/kaggle/working/') \nimport zipfile\nwith zipfile.ZipFile(\"/kaggle/working/corpora/wordnet.zip\", 'r') as zip_f:\n    zip_f.extractall(\"/kaggle/working/corpora/\")\nwarnings.filterwarnings(\"ignore\")\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('dark_background')\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-02T06:24:23.436371Z","iopub.execute_input":"2023-07-02T06:24:23.4372Z","iopub.status.idle":"2023-07-02T06:24:23.782714Z","shell.execute_reply.started":"2023-07-02T06:24:23.437163Z","shell.execute_reply":"2023-07-02T06:24:23.781562Z"},"trusted":true},"execution_count":176,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /kaggle/working/...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /kaggle/working/...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /kaggle/working/...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/ted-ultimate-dataset/2020-05-01/ted_talks_en.csv\")\ndf.sort_values(by=\"views\",ascending=False,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:24:23.784875Z","iopub.execute_input":"2023-07-02T06:24:23.785222Z","iopub.status.idle":"2023-07-02T06:24:24.485154Z","shell.execute_reply.started":"2023-07-02T06:24:23.785193Z","shell.execute_reply":"2023-07-02T06:24:24.483933Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"code","source":"# Tim Urban: \"Inside the mind of a master procrastinator\" Speech                                            \nspeech1 = df.iloc[6].transcript","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:24:24.486421Z","iopub.execute_input":"2023-07-02T06:24:24.486778Z","iopub.status.idle":"2023-07-02T06:24:24.492518Z","shell.execute_reply.started":"2023-07-02T06:24:24.486749Z","shell.execute_reply":"2023-07-02T06:24:24.491414Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":"# Stephen Hawking: “Questioning the Universe* Speech\nspeech2 = \"\"\"There is nothing bigger or older than the universe. \nThe questions I would like to talk about are: one, where did we come from?\nHow did the universe come into being? Are we alone in the universe?\nIs there alien life out there? What is the future of the human race?\nUp until the 1920s, everyone thought the universe was essentially static and unchanging in time.\nThen it was discovered that the universe was expanding. Distant galaxies were moving away from us.\nThis meant they must have been closer together in the past. If we extrapolate back, \nwe find we must have all been on top of each other about 15 billion years ago. \nThis was the Big Bang, the beginning of the universe. But was there anything before the Big Bang?\nIf not, what created the universe? Why did the universe emerge from the Big Bang the way it did?\nWe used to think that the theory of the universe could be divided into two parts. \nFirst, there were the laws like Maxwell’s equations and general relativity that determined the\nevolution of the universe, given its state over all of the space at one time. And second, \nthere was no question of the initial state of the universe. We have made good progress on the\nfirst part, and now have the knowledge of the laws of evolution in all but the most extreme conditions.\nBut until recently, we have had little idea about the initial conditions for the universe.\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:24:24.495131Z","iopub.execute_input":"2023-07-02T06:24:24.495607Z","iopub.status.idle":"2023-07-02T06:24:24.511683Z","shell.execute_reply.started":"2023-07-02T06:24:24.49557Z","shell.execute_reply":"2023-07-02T06:24:24.51073Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# 1. Tokenization\n### 1.1 Sentence Tokenization ","metadata":{}},{"cell_type":"code","source":"sentences = nltk.sent_tokenize(speech2)\nsentences[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:24:53.89431Z","iopub.execute_input":"2023-07-02T06:24:53.894777Z","iopub.status.idle":"2023-07-02T06:24:53.903068Z","shell.execute_reply.started":"2023-07-02T06:24:53.894744Z","shell.execute_reply":"2023-07-02T06:24:53.901554Z"},"trusted":true},"execution_count":193,"outputs":[{"execution_count":193,"output_type":"execute_result","data":{"text/plain":"['There is nothing bigger or older than the universe.',\n 'The questions I would like to talk about are: one, where did we come from?',\n 'How did the universe come into being?',\n 'Are we alone in the universe?',\n 'Is there alien life out there?']"},"metadata":{}}]},{"cell_type":"code","source":"sentences = nltk.sent_tokenize(speech1)\nsentences[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:24:57.994513Z","iopub.execute_input":"2023-07-02T06:24:57.995229Z","iopub.status.idle":"2023-07-02T06:24:58.008118Z","shell.execute_reply.started":"2023-07-02T06:24:57.995191Z","shell.execute_reply":"2023-07-02T06:24:58.006758Z"},"trusted":true},"execution_count":194,"outputs":[{"execution_count":194,"output_type":"execute_result","data":{"text/plain":"['So in college, I was a government major, which means I had to write a lot of papers.',\n 'Now, when a normal student writes a paper, they might spread the work out a little like this.',\n 'So, you know — (Laughter) you get started maybe a little slowly, but you get enough done in the first week that, with some heavier days later on, everything gets done, things stay civil.',\n '(Laughter) And I would want to do that like that.',\n 'That would be the plan.']"},"metadata":{}}]},{"cell_type":"markdown","source":"### 1.2 Word Tokenization ","metadata":{}},{"cell_type":"code","source":"words = nltk.word_tokenize(speech1)\nlen(words)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:25:24.419618Z","iopub.execute_input":"2023-07-02T06:25:24.420015Z","iopub.status.idle":"2023-07-02T06:25:24.449432Z","shell.execute_reply.started":"2023-07-02T06:25:24.419982Z","shell.execute_reply":"2023-07-02T06:25:24.448098Z"},"trusted":true},"execution_count":199,"outputs":[{"execution_count":199,"output_type":"execute_result","data":{"text/plain":"2769"},"metadata":{}}]},{"cell_type":"code","source":"for i in range(random.randint(1,50),random.randint(100,200)):\n    print(words[i],end=\" \")","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:25:25.75884Z","iopub.execute_input":"2023-07-02T06:25:25.759319Z","iopub.status.idle":"2023-07-02T06:25:25.767249Z","shell.execute_reply.started":"2023-07-02T06:25:25.759281Z","shell.execute_reply":"2023-07-02T06:25:25.76607Z"},"trusted":true},"execution_count":200,"outputs":[{"name":"stdout","text":"a lot of papers . Now , when a normal student writes a paper , they might spread the work out a little like this . So , you know — ( Laughter ) you get started maybe a little slowly , but you get enough done in the first week that , with some heavier days later on , everything gets done , things stay civil . ( Laughter ) And I would want to do that like that . That would be the plan . I would have it all ready to go , but then , actually , the paper would come along , and then I would kind of do this . ( Laughter ) And that would happen every single paper . But then came my 90-page senior thesis , a paper you 're supposed to spend a year on . And I knew for ","output_type":"stream"}]},{"cell_type":"markdown","source":"---\n\n# 2. Stemming vs Lemmatization\n\n### 2.1 Stemming","metadata":{}},{"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words(\"english\")\nprint(stopwords)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:24:24.598299Z","iopub.execute_input":"2023-07-02T06:24:24.598818Z","iopub.status.idle":"2023-07-02T06:24:24.616166Z","shell.execute_reply.started":"2023-07-02T06:24:24.598784Z","shell.execute_reply":"2023-07-02T06:24:24.614599Z"},"trusted":true},"execution_count":184,"outputs":[{"name":"stdout","text":"['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","output_type":"stream"}]},{"cell_type":"code","source":"stemmer = nltk.PorterStemmer()\nstemmedSentences = []\n\nfor i in range(len(sentences)):\n    words = nltk.word_tokenize(sentences[i])\n#     words = [re.sub('[!,*)@#%(&$_?.^]',\"\",i).lower() for i in words]\n    words = [re.sub(\"[^a-zA-Z0-9]\",\"\",i).lower().lstrip() for i in words]\n    words = [stemmer.stem(i) for i in words if i not in stopwords]\n    stemmedSentences.append(\" \".join(words))\nstemmedSentences[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:25:43.844688Z","iopub.execute_input":"2023-07-02T06:25:43.845165Z","iopub.status.idle":"2023-07-02T06:25:43.925372Z","shell.execute_reply.started":"2023-07-02T06:25:43.84513Z","shell.execute_reply":"2023-07-02T06:25:43.924227Z"},"trusted":true},"execution_count":201,"outputs":[{"execution_count":201,"output_type":"execute_result","data":{"text/plain":"['colleg  govern major  mean write lot paper ',\n ' normal student write paper  might spread work littl like ',\n ' know   laughter  get start mayb littl slowli  get enough done first week  heavier day later  everyth get done  thing stay civil ',\n ' laughter  would want like ',\n 'would plan ']"},"metadata":{}}]},{"cell_type":"markdown","source":"### 2.2 Lemmatization ","metadata":{}},{"cell_type":"code","source":"lemmatizer = nltk.stem.WordNetLemmatizer()\nlemmatizedSentences = []\n\nfor i in range(len(sentences)):\n    words = nltk.word_tokenize(sentences[i])\n#     words = [re.sub(\"[!,*)@#%(&$_?.:’'^]\",\"\",i).lower() for i in words]\n    words = [re.sub(\"[^a-zA-Z0-9]\",\"\",i).lower().strip() for i in words]\n    words = [lemmatizer.lemmatize(i) for i in words if i not in stopwords]\n    lemmatizedSentences.append(\" \".join(words))\nlemmatizedSentences[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:25:52.982566Z","iopub.execute_input":"2023-07-02T06:25:52.98294Z","iopub.status.idle":"2023-07-02T06:25:53.039067Z","shell.execute_reply.started":"2023-07-02T06:25:52.982913Z","shell.execute_reply":"2023-07-02T06:25:53.037832Z"},"trusted":true},"execution_count":202,"outputs":[{"execution_count":202,"output_type":"execute_result","data":{"text/plain":"['college  government major  mean write lot paper ',\n ' normal student writes paper  might spread work little like ',\n ' know   laughter  get started maybe little slowly  get enough done first week  heavier day later  everything get done  thing stay civil ',\n ' laughter  would want like ',\n 'would plan ']"},"metadata":{}}]},{"cell_type":"markdown","source":"### 2.3 Comparing Stemming vs Lemmatization","metadata":{}},{"cell_type":"code","source":"print(stemmedSentences[6])\nprint(lemmatizedSentences[6])","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:25:55.744563Z","iopub.execute_input":"2023-07-02T06:25:55.744934Z","iopub.status.idle":"2023-07-02T06:25:55.75111Z","shell.execute_reply.started":"2023-07-02T06:25:55.744905Z","shell.execute_reply":"2023-07-02T06:25:55.749649Z"},"trusted":true},"execution_count":203,"outputs":[{"name":"stdout","text":" laughter  would happen everi singl paper \n laughter  would happen every single paper \n","output_type":"stream"}]},{"cell_type":"markdown","source":"---\n\n# 3. Vectorization\n### 3.1 Bag of Words (CountVectorizer)\n- sklearn.feature_extraction.text.CountVectorizer","metadata":{}},{"cell_type":"code","source":"# Frequncy BoW\ncountVectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features=2000)\nX = countVectorizer.fit_transform(lemmatizedSentences).toarray() \nX.shape\n# 20 - no of sentences\n# 78 - no of features","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:26:02.167544Z","iopub.execute_input":"2023-07-02T06:26:02.167958Z","iopub.status.idle":"2023-07-02T06:26:02.181302Z","shell.execute_reply.started":"2023-07-02T06:26:02.167916Z","shell.execute_reply":"2023-07-02T06:26:02.179852Z"},"trusted":true},"execution_count":204,"outputs":[{"execution_count":204,"output_type":"execute_result","data":{"text/plain":"(142, 482)"},"metadata":{}}]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:26:04.667894Z","iopub.execute_input":"2023-07-02T06:26:04.668298Z","iopub.status.idle":"2023-07-02T06:26:04.674485Z","shell.execute_reply.started":"2023-07-02T06:26:04.668266Z","shell.execute_reply":"2023-07-02T06:26:04.673567Z"},"trusted":true},"execution_count":205,"outputs":[{"name":"stdout","text":"[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 3.2 TF-IDF (Term Frequency * Inverse Document Frequency)\n - sklearn.feature_extraction.text.TfidfVectorizer","metadata":{}},{"cell_type":"code","source":"tfidfVecorier = sklearn.feature_extraction.text.TfidfVectorizer(max_features=2000)\nX = tfidfVecorier.fit_transform(lemmatizedSentences).toarray() \nX.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:26:06.510532Z","iopub.execute_input":"2023-07-02T06:26:06.510972Z","iopub.status.idle":"2023-07-02T06:26:06.526667Z","shell.execute_reply.started":"2023-07-02T06:26:06.510941Z","shell.execute_reply":"2023-07-02T06:26:06.524875Z"},"trusted":true},"execution_count":206,"outputs":[{"execution_count":206,"output_type":"execute_result","data":{"text/plain":"(142, 482)"},"metadata":{}}]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:26:08.348802Z","iopub.execute_input":"2023-07-02T06:26:08.349331Z","iopub.status.idle":"2023-07-02T06:26:08.357928Z","shell.execute_reply.started":"2023-07-02T06:26:08.349294Z","shell.execute_reply":"2023-07-02T06:26:08.356325Z"},"trusted":true},"execution_count":207,"outputs":[{"name":"stdout","text":"[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n","output_type":"stream"}]}]}