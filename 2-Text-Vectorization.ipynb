{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/iqmansingh/getting-started-with-nlp?scriptVersionId=135491469\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"c0548fdc","metadata":{"papermill":{"duration":0.008735,"end_time":"2023-07-02T09:52:02.725297","exception":false,"start_time":"2023-07-02T09:52:02.716562","status":"completed"},"tags":[]},"source":["<img src=\"https://cdn.discordapp.com/attachments/1111599839663370271/1124998618529681428/NLP-Banner.jpg\">\n","\n","# **Getting Started with NLP**\n","\n","---"]},{"cell_type":"code","execution_count":1,"id":"4340d1d4","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-02T09:52:02.743853Z","iopub.status.busy":"2023-07-02T09:52:02.743297Z","iopub.status.idle":"2023-07-02T09:52:16.580104Z","shell.execute_reply":"2023-07-02T09:52:16.578828Z"},"papermill":{"duration":13.849901,"end_time":"2023-07-02T09:52:16.583288","exception":false,"start_time":"2023-07-02T09:52:02.733387","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"]},{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /kaggle/working/...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /kaggle/working/...\n","[nltk_data] Downloading package stopwords to /kaggle/working/...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["import numpy as np\n","import pandas as pd \n","import tensorflow as tf\n","import datetime\n","import warnings\n","import nltk\n","import random\n","import re\n","import sklearn\n","nltk.download('punkt',download_dir=\"/kaggle/working/\")\n","nltk.download('wordnet',download_dir=\"/kaggle/working/\")\n","nltk.download('stopwords',download_dir=\"/kaggle/working/\")\n","nltk.data.path.append('/kaggle/working/') \n","import zipfile\n","with zipfile.ZipFile(\"/kaggle/working/corpora/wordnet.zip\", 'r') as zip_f:\n","    zip_f.extractall(\"/kaggle/working/corpora/\")\n","warnings.filterwarnings(\"ignore\")\n","pd.plotting.register_matplotlib_converters()\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","plt.style.use('dark_background')\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"id":"886ccc1c","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:16.602841Z","iopub.status.busy":"2023-07-02T09:52:16.601939Z","iopub.status.idle":"2023-07-02T09:52:17.840524Z","shell.execute_reply":"2023-07-02T09:52:17.838746Z"},"papermill":{"duration":1.253151,"end_time":"2023-07-02T09:52:17.845098","exception":false,"start_time":"2023-07-02T09:52:16.591947","status":"completed"},"tags":[]},"outputs":[],"source":["df = pd.read_csv(\"/kaggle/input/ted-ultimate-dataset/2020-05-01/ted_talks_en.csv\")\n","df.sort_values(by=\"views\",ascending=False,inplace=True)"]},{"cell_type":"code","execution_count":3,"id":"8f03f934","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:17.882725Z","iopub.status.busy":"2023-07-02T09:52:17.882052Z","iopub.status.idle":"2023-07-02T09:52:17.890659Z","shell.execute_reply":"2023-07-02T09:52:17.889681Z"},"papermill":{"duration":0.030411,"end_time":"2023-07-02T09:52:17.894288","exception":false,"start_time":"2023-07-02T09:52:17.863877","status":"completed"},"tags":[]},"outputs":[],"source":["# Tim Urban: \"Inside the mind of a master procrastinator\" Speech                                            \n","speech1 = df.iloc[6].transcript"]},{"cell_type":"code","execution_count":4,"id":"c313c718","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:17.924423Z","iopub.status.busy":"2023-07-02T09:52:17.923942Z","iopub.status.idle":"2023-07-02T09:52:17.9316Z","shell.execute_reply":"2023-07-02T09:52:17.930169Z"},"papermill":{"duration":0.022608,"end_time":"2023-07-02T09:52:17.934089","exception":false,"start_time":"2023-07-02T09:52:17.911481","status":"completed"},"tags":[]},"outputs":[],"source":["# Stephen Hawking: “Questioning the Universe* Speech\n","speech2 = \"\"\"There is nothing bigger or older than the universe. \n","The questions I would like to talk about are: one, where did we come from?\n","How did the universe come into being? Are we alone in the universe?\n","Is there alien life out there? What is the future of the human race?\n","Up until the 1920s, everyone thought the universe was essentially static and unchanging in time.\n","Then it was discovered that the universe was expanding. Distant galaxies were moving away from us.\n","This meant they must have been closer together in the past. If we extrapolate back, \n","we find we must have all been on top of each other about 15 billion years ago. \n","This was the Big Bang, the beginning of the universe. But was there anything before the Big Bang?\n","If not, what created the universe? Why did the universe emerge from the Big Bang the way it did?\n","We used to think that the theory of the universe could be divided into two parts. \n","First, there were the laws like Maxwell’s equations and general relativity that determined the\n","evolution of the universe, given its state over all of the space at one time. And second, \n","there was no question of the initial state of the universe. We have made good progress on the\n","first part, and now have the knowledge of the laws of evolution in all but the most extreme conditions.\n","But until recently, we have had little idea about the initial conditions for the universe.\"\"\""]},{"cell_type":"markdown","id":"2f4004b1","metadata":{"papermill":{"duration":0.007491,"end_time":"2023-07-02T09:52:17.949422","exception":false,"start_time":"2023-07-02T09:52:17.941931","status":"completed"},"tags":[]},"source":["---\n","\n","# 1. Tokenization\n","### 1.1 Sentence Tokenization "]},{"cell_type":"code","execution_count":5,"id":"7b142e59","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:17.967765Z","iopub.status.busy":"2023-07-02T09:52:17.967267Z","iopub.status.idle":"2023-07-02T09:52:17.994659Z","shell.execute_reply":"2023-07-02T09:52:17.993313Z"},"papermill":{"duration":0.039831,"end_time":"2023-07-02T09:52:17.997173","exception":false,"start_time":"2023-07-02T09:52:17.957342","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["['There is nothing bigger or older than the universe.',\n"," 'The questions I would like to talk about are: one, where did we come from?',\n"," 'How did the universe come into being?',\n"," 'Are we alone in the universe?',\n"," 'Is there alien life out there?']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["sentences = nltk.sent_tokenize(speech2)\n","sentences[:5]"]},{"cell_type":"code","execution_count":6,"id":"38c67318","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:18.015361Z","iopub.status.busy":"2023-07-02T09:52:18.014914Z","iopub.status.idle":"2023-07-02T09:52:18.031633Z","shell.execute_reply":"2023-07-02T09:52:18.03051Z"},"papermill":{"duration":0.02898,"end_time":"2023-07-02T09:52:18.034178","exception":false,"start_time":"2023-07-02T09:52:18.005198","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["['So in college, I was a government major, which means I had to write a lot of papers.',\n"," 'Now, when a normal student writes a paper, they might spread the work out a little like this.',\n"," 'So, you know — (Laughter) you get started maybe a little slowly, but you get enough done in the first week that, with some heavier days later on, everything gets done, things stay civil.',\n"," '(Laughter) And I would want to do that like that.',\n"," 'That would be the plan.']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["sentences = nltk.sent_tokenize(speech1)\n","sentences[:5]"]},{"cell_type":"markdown","id":"883abfa3","metadata":{"papermill":{"duration":0.007977,"end_time":"2023-07-02T09:52:18.050525","exception":false,"start_time":"2023-07-02T09:52:18.042548","status":"completed"},"tags":[]},"source":["### 1.2 Word Tokenization "]},{"cell_type":"code","execution_count":7,"id":"aa58c0ea","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:18.069988Z","iopub.status.busy":"2023-07-02T09:52:18.069431Z","iopub.status.idle":"2023-07-02T09:52:18.116784Z","shell.execute_reply":"2023-07-02T09:52:18.115559Z"},"papermill":{"duration":0.060613,"end_time":"2023-07-02T09:52:18.119408","exception":false,"start_time":"2023-07-02T09:52:18.058795","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["2769"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["words = nltk.word_tokenize(speech1)\n","len(words)"]},{"cell_type":"code","execution_count":8,"id":"028d0446","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:18.139283Z","iopub.status.busy":"2023-07-02T09:52:18.138024Z","iopub.status.idle":"2023-07-02T09:52:18.147038Z","shell.execute_reply":"2023-07-02T09:52:18.145529Z"},"papermill":{"duration":0.021592,"end_time":"2023-07-02T09:52:18.149659","exception":false,"start_time":"2023-07-02T09:52:18.128067","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["little like this . So , you know — ( Laughter ) you get started maybe a little slowly , but you get enough done in the first week that , with some heavier days later on , everything gets done , things stay civil . ( Laughter ) And I would want to do that like that . That would be the plan . I would have it all ready to go , but then , actually , the paper would come along , and then I would kind of do this . ( Laughter ) And that would happen every single paper . But then came my 90-page senior thesis , a paper you 're supposed to spend "]}],"source":["for i in range(random.randint(1,50),random.randint(100,200)):\n","    print(words[i],end=\" \")"]},{"cell_type":"markdown","id":"f1f765d2","metadata":{"papermill":{"duration":0.008034,"end_time":"2023-07-02T09:52:18.16615","exception":false,"start_time":"2023-07-02T09:52:18.158116","status":"completed"},"tags":[]},"source":["---\n","\n","# 2. Stemming vs Lemmatization\n","\n","### 2.1 Stemming"]},{"cell_type":"code","execution_count":9,"id":"ebf58eb4","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:18.185952Z","iopub.status.busy":"2023-07-02T09:52:18.185521Z","iopub.status.idle":"2023-07-02T09:52:18.197581Z","shell.execute_reply":"2023-07-02T09:52:18.195972Z"},"papermill":{"duration":0.02558,"end_time":"2023-07-02T09:52:18.200633","exception":false,"start_time":"2023-07-02T09:52:18.175053","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}],"source":["stopwords = nltk.corpus.stopwords.words(\"english\")\n","print(stopwords)"]},{"cell_type":"code","execution_count":10,"id":"f748c495","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:18.220298Z","iopub.status.busy":"2023-07-02T09:52:18.219867Z","iopub.status.idle":"2023-07-02T09:52:18.32507Z","shell.execute_reply":"2023-07-02T09:52:18.323895Z"},"papermill":{"duration":0.118227,"end_time":"2023-07-02T09:52:18.327515","exception":false,"start_time":"2023-07-02T09:52:18.209288","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["['colleg  govern major  mean write lot paper ',\n"," ' normal student write paper  might spread work littl like ',\n"," ' know   laughter  get start mayb littl slowli  get enough done first week  heavier day later  everyth get done  thing stay civil ',\n"," ' laughter  would want like ',\n"," 'would plan ']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["stemmer = nltk.PorterStemmer()\n","stemmedSentences = []\n","\n","for i in range(len(sentences)):\n","    words = nltk.word_tokenize(sentences[i])\n","#     words = [re.sub('[!,*)@#%(&$_?.^]',\"\",i).lower() for i in words]\n","    words = [re.sub(\"[^a-zA-Z0-9]\",\"\",i).lower().lstrip() for i in words]\n","    words = [stemmer.stem(i) for i in words if i not in stopwords]\n","    stemmedSentences.append(\" \".join(words))\n","stemmedSentences[:5]"]},{"cell_type":"markdown","id":"02d5460c","metadata":{"papermill":{"duration":0.008675,"end_time":"2023-07-02T09:52:18.345065","exception":false,"start_time":"2023-07-02T09:52:18.33639","status":"completed"},"tags":[]},"source":["### 2.2 Lemmatization "]},{"cell_type":"code","execution_count":11,"id":"ef648453","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:18.36669Z","iopub.status.busy":"2023-07-02T09:52:18.365512Z","iopub.status.idle":"2023-07-02T09:52:21.385321Z","shell.execute_reply":"2023-07-02T09:52:21.383719Z"},"papermill":{"duration":3.033767,"end_time":"2023-07-02T09:52:21.388156","exception":false,"start_time":"2023-07-02T09:52:18.354389","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["['college  government major  mean write lot paper ',\n"," ' normal student writes paper  might spread work little like ',\n"," ' know   laughter  get started maybe little slowly  get enough done first week  heavier day later  everything get done  thing stay civil ',\n"," ' laughter  would want like ',\n"," 'would plan ']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["lemmatizer = nltk.stem.WordNetLemmatizer()\n","lemmatizedSentences = []\n","\n","for i in range(len(sentences)):\n","    words = nltk.word_tokenize(sentences[i])\n","#     words = [re.sub(\"[!,*)@#%(&$_?.:’'^]\",\"\",i).lower() for i in words]\n","    words = [re.sub(\"[^a-zA-Z0-9]\",\"\",i).lower().strip() for i in words]\n","    words = [lemmatizer.lemmatize(i) for i in words if i not in stopwords]\n","    lemmatizedSentences.append(\" \".join(words))\n","lemmatizedSentences[:5]"]},{"cell_type":"markdown","id":"387df227","metadata":{"papermill":{"duration":0.008549,"end_time":"2023-07-02T09:52:21.405899","exception":false,"start_time":"2023-07-02T09:52:21.39735","status":"completed"},"tags":[]},"source":["### 2.3 Comparing Stemming vs Lemmatization"]},{"cell_type":"code","execution_count":12,"id":"185fd602","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:21.425411Z","iopub.status.busy":"2023-07-02T09:52:21.42502Z","iopub.status.idle":"2023-07-02T09:52:21.431723Z","shell.execute_reply":"2023-07-02T09:52:21.430111Z"},"papermill":{"duration":0.019601,"end_time":"2023-07-02T09:52:21.434406","exception":false,"start_time":"2023-07-02T09:52:21.414805","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[" laughter  would happen everi singl paper \n"," laughter  would happen every single paper \n"]}],"source":["print(stemmedSentences[6])\n","print(lemmatizedSentences[6])"]},{"cell_type":"markdown","id":"6a1f5357","metadata":{"papermill":{"duration":0.008529,"end_time":"2023-07-02T09:52:21.451686","exception":false,"start_time":"2023-07-02T09:52:21.443157","status":"completed"},"tags":[]},"source":["---\n","\n","# 3. Vectorization\n","### 3.1 Bag of Words (CountVectorizer)\n","- sklearn.feature_extraction.text.CountVectorizer"]},{"cell_type":"code","execution_count":13,"id":"6aff9b57","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:21.471796Z","iopub.status.busy":"2023-07-02T09:52:21.471323Z","iopub.status.idle":"2023-07-02T09:52:21.492815Z","shell.execute_reply":"2023-07-02T09:52:21.491783Z"},"papermill":{"duration":0.034583,"end_time":"2023-07-02T09:52:21.495235","exception":false,"start_time":"2023-07-02T09:52:21.460652","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(142, 482)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Frequncy BoW\n","countVectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features=2000)\n","X = countVectorizer.fit_transform(lemmatizedSentences).toarray() \n","X.shape\n","# 20 - no of sentences\n","# 78 - no of features"]},{"cell_type":"code","execution_count":14,"id":"59eb02ab","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:21.516785Z","iopub.status.busy":"2023-07-02T09:52:21.51602Z","iopub.status.idle":"2023-07-02T09:52:21.521411Z","shell.execute_reply":"2023-07-02T09:52:21.520638Z"},"papermill":{"duration":0.01902,"end_time":"2023-07-02T09:52:21.524028","exception":false,"start_time":"2023-07-02T09:52:21.505008","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," ...\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]]\n"]}],"source":["print(X)"]},{"cell_type":"markdown","id":"cda4dbea","metadata":{"papermill":{"duration":0.009022,"end_time":"2023-07-02T09:52:21.542912","exception":false,"start_time":"2023-07-02T09:52:21.53389","status":"completed"},"tags":[]},"source":["### 3.2 TF-IDF (Term Frequency * Inverse Document Frequency)\n"," - sklearn.feature_extraction.text.TfidfVectorizer"]},{"cell_type":"code","execution_count":15,"id":"4a526a91","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:21.565068Z","iopub.status.busy":"2023-07-02T09:52:21.564209Z","iopub.status.idle":"2023-07-02T09:52:21.589787Z","shell.execute_reply":"2023-07-02T09:52:21.588862Z"},"papermill":{"duration":0.03963,"end_time":"2023-07-02T09:52:21.592445","exception":false,"start_time":"2023-07-02T09:52:21.552815","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(142, 482)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["tfidfVecorier = sklearn.feature_extraction.text.TfidfVectorizer(max_features=2000)\n","X = tfidfVecorier.fit_transform(lemmatizedSentences).toarray() \n","X.shape"]},{"cell_type":"code","execution_count":16,"id":"faf1e83d","metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:52:21.614396Z","iopub.status.busy":"2023-07-02T09:52:21.613663Z","iopub.status.idle":"2023-07-02T09:52:21.61981Z","shell.execute_reply":"2023-07-02T09:52:21.618981Z"},"papermill":{"duration":0.02054,"end_time":"2023-07-02T09:52:21.622867","exception":false,"start_time":"2023-07-02T09:52:21.602327","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"]}],"source":["print(X)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"papermill":{"default_parameters":{},"duration":35.897853,"end_time":"2023-07-02T09:52:24.969437","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-07-02T09:51:49.071584","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}