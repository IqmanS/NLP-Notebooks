{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a23ec88",
   "metadata": {
    "papermill": {
     "duration": 0.004337,
     "end_time": "2023-07-02T04:56:55.517061",
     "exception": false,
     "start_time": "2023-07-02T04:56:55.512724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Getting Started with NLP**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acd45ce0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-02T04:56:55.526698Z",
     "iopub.status.busy": "2023-07-02T04:56:55.525922Z",
     "iopub.status.idle": "2023-07-02T04:57:07.719724Z",
     "shell.execute_reply": "2023-07-02T04:57:07.718597Z"
    },
    "papermill": {
     "duration": 12.201298,
     "end_time": "2023-07-02T04:57:07.722239",
     "exception": false,
     "start_time": "2023-07-02T04:56:55.520941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /kaggle/working/...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /kaggle/working/...\n",
      "[nltk_data] Downloading package stopwords to /kaggle/working/...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import warnings\n",
    "import nltk\n",
    "import random\n",
    "nltk.download('punkt',download_dir=\"/kaggle/working/\")\n",
    "nltk.download('wordnet',download_dir=\"/kaggle/working/\")\n",
    "nltk.download('stopwords',download_dir=\"/kaggle/working/\")\n",
    "nltk.data.path.append('/kaggle/working/') \n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"/kaggle/working/corpora/wordnet.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"/kaggle/working/corpora/\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.style.use('dark_background')\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3296826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T04:57:07.732749Z",
     "iopub.status.busy": "2023-07-02T04:57:07.732065Z",
     "iopub.status.idle": "2023-07-02T04:57:07.738512Z",
     "shell.execute_reply": "2023-07-02T04:57:07.737372Z"
    },
    "papermill": {
     "duration": 0.013895,
     "end_time": "2023-07-02T04:57:07.740643",
     "exception": false,
     "start_time": "2023-07-02T04:57:07.726748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stephen Hawking: “Questioning the Universe* Speech/Corpus\n",
    "speech = \"\"\"There is nothing bigger or older than the universe. \n",
    "The questions I would like to talk about are: one, where did we come from?\n",
    "How did the universe come into being? Are we alone in the universe?\n",
    "Is there alien life out there? What is the future of the human race?\n",
    "Up until the 1920s, everyone thought the universe was essentially static and unchanging in time.\n",
    "Then it was discovered that the universe was expanding. Distant galaxies were moving away from us.\n",
    "This meant they must have been closer together in the past. If we extrapolate back, \n",
    "we find we must have all been on top of each other about 15 billion years ago. \n",
    "This was the Big Bang, the beginning of the universe. But was there anything before the Big Bang?\n",
    "If not, what created the universe? Why did the universe emerge from the Big Bang the way it did?\n",
    "We used to think that the theory of the universe could be divided into two parts. \n",
    "First, there were the laws like Maxwell’s equations and general relativity that determined the\n",
    "evolution of the universe, given its state over all of the space at one time. And second, \n",
    "there was no question of the initial state of the universe. We have made good progress on the\n",
    "first part, and now have the knowledge of the laws of evolution in all but the most extreme conditions.\n",
    "But until recently, we have had little idea about the initial conditions for the universe.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eeec23",
   "metadata": {
    "papermill": {
     "duration": 0.003837,
     "end_time": "2023-07-02T04:57:07.748664",
     "exception": false,
     "start_time": "2023-07-02T04:57:07.744827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Tokenization\n",
    "### 1.1 Sentence Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c50413",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T04:57:07.758990Z",
     "iopub.status.busy": "2023-07-02T04:57:07.758606Z",
     "iopub.status.idle": "2023-07-02T04:57:07.779668Z",
     "shell.execute_reply": "2023-07-02T04:57:07.778846Z"
    },
    "papermill": {
     "duration": 0.028944,
     "end_time": "2023-07-02T04:57:07.782017",
     "exception": false,
     "start_time": "2023-07-02T04:57:07.753073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is nothing bigger or older than the universe.',\n",
       " 'The questions I would like to talk about are: one, where did we come from?',\n",
       " 'How did the universe come into being?',\n",
       " 'Are we alone in the universe?',\n",
       " 'Is there alien life out there?',\n",
       " 'What is the future of the human race?',\n",
       " 'Up until the 1920s, everyone thought the universe was essentially static and unchanging in time.',\n",
       " 'Then it was discovered that the universe was expanding.',\n",
       " 'Distant galaxies were moving away from us.',\n",
       " 'This meant they must have been closer together in the past.',\n",
       " 'If we extrapolate back, \\nwe find we must have all been on top of each other about 15 billion years ago.',\n",
       " 'This was the Big Bang, the beginning of the universe.',\n",
       " 'But was there anything before the Big Bang?',\n",
       " 'If not, what created the universe?',\n",
       " 'Why did the universe emerge from the Big Bang the way it did?',\n",
       " 'We used to think that the theory of the universe could be divided into two parts.',\n",
       " 'First, there were the laws like Maxwell’s equations and general relativity that determined the\\nevolution of the universe, given its state over all of the space at one time.',\n",
       " 'And second, \\nthere was no question of the initial state of the universe.',\n",
       " 'We have made good progress on the\\nfirst part, and now have the knowledge of the laws of evolution in all but the most extreme conditions.',\n",
       " 'But until recently, we have had little idea about the initial conditions for the universe.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(speech)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5557e0",
   "metadata": {
    "papermill": {
     "duration": 0.003976,
     "end_time": "2023-07-02T04:57:07.790460",
     "exception": false,
     "start_time": "2023-07-02T04:57:07.786484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.2 Word Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "423325ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T04:57:07.800488Z",
     "iopub.status.busy": "2023-07-02T04:57:07.800088Z",
     "iopub.status.idle": "2023-07-02T04:57:07.808792Z",
     "shell.execute_reply": "2023-07-02T04:57:07.807844Z"
    },
    "papermill": {
     "duration": 0.016267,
     "end_time": "2023-07-02T04:57:07.810852",
     "exception": false,
     "start_time": "2023-07-02T04:57:07.794585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(speech)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74dd6473",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T04:57:07.823807Z",
     "iopub.status.busy": "2023-07-02T04:57:07.823432Z",
     "iopub.status.idle": "2023-07-02T04:57:07.828885Z",
     "shell.execute_reply": "2023-07-02T04:57:07.827911Z"
    },
    "papermill": {
     "duration": 0.014054,
     "end_time": "2023-07-02T04:57:07.831122",
     "exception": false,
     "start_time": "2023-07-02T04:57:07.817068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one / , / where / did / we / come / from / ? / How / did / the / universe / come / into / being / ? / Are / we / alone / in / the / universe / ? / Is / there / alien / life / out / there / ? / What / is / the / future / of / the / human / race / ? / Up / until / the / 1920s / , / everyone / thought / the / universe / was / essentially / static / and / unchanging / in / time / . / Then / it / was / discovered / "
     ]
    }
   ],
   "source": [
    "for i in range(random.randint(1,50),random.randint(50,100)):\n",
    "    print(words[i],end=\" / \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9044b6b1",
   "metadata": {
    "papermill": {
     "duration": 0.004104,
     "end_time": "2023-07-02T04:57:07.839750",
     "exception": false,
     "start_time": "2023-07-02T04:57:07.835646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24464d80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T04:57:07.850298Z",
     "iopub.status.busy": "2023-07-02T04:57:07.849929Z",
     "iopub.status.idle": "2023-07-02T04:57:07.859651Z",
     "shell.execute_reply": "2023-07-02T04:57:07.858467Z"
    },
    "papermill": {
     "duration": 0.017482,
     "end_time": "2023-07-02T04:57:07.861644",
     "exception": false,
     "start_time": "2023-07-02T04:57:07.844162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d70033d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T04:57:07.872088Z",
     "iopub.status.busy": "2023-07-02T04:57:07.871720Z",
     "iopub.status.idle": "2023-07-02T04:57:07.886210Z",
     "shell.execute_reply": "2023-07-02T04:57:07.885045Z"
    },
    "papermill": {
     "duration": 0.022396,
     "end_time": "2023-07-02T04:57:07.888518",
     "exception": false,
     "start_time": "2023-07-02T04:57:07.866122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there noth bigger older univers .\n",
      "the question I would like talk : one , come ?\n",
      "how univers come ?\n",
      "are alon univers ?\n",
      "Is alien life ?\n",
      "what futur human race ?\n",
      "Up 1920 , everyon thought univers essenti static unchang time .\n",
      "then discov univers expand .\n",
      "distant galaxi move away us .\n",
      "thi meant must closer togeth past .\n",
      "If extrapol back , find must top 15 billion year ago .\n",
      "thi big bang , begin univers .\n",
      "but anyth big bang ?\n",
      "If , creat univers ?\n",
      "whi univers emerg big bang way ?\n",
      "We use think theori univers could divid two part .\n",
      "first , law like maxwel ’ equat gener rel determin evolut univers , given state space one time .\n",
      "and second , question initi state univers .\n",
      "We made good progress first part , knowledg law evolut extrem condit .\n",
      "but recent , littl idea initi condit univers .\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "stemmed_sentences = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(i) for i in words if i not in stopwords]\n",
    "    stemmed_sentences.append(\" \".join(words))\n",
    "    print(stemmed_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f384aa",
   "metadata": {
    "papermill": {
     "duration": 0.004353,
     "end_time": "2023-07-02T04:57:07.897583",
     "exception": false,
     "start_time": "2023-07-02T04:57:07.893230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423932ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T04:57:07.908529Z",
     "iopub.status.busy": "2023-07-02T04:57:07.908108Z",
     "iopub.status.idle": "2023-07-02T04:57:09.819117Z",
     "shell.execute_reply": "2023-07-02T04:57:09.818076Z"
    },
    "papermill": {
     "duration": 1.919664,
     "end_time": "2023-07-02T04:57:09.821780",
     "exception": false,
     "start_time": "2023-07-02T04:57:07.902116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There nothing bigger older universe .\n",
      "The question I would like talk : one , come ?\n",
      "How universe come ?\n",
      "Are alone universe ?\n",
      "Is alien life ?\n",
      "What future human race ?\n",
      "Up 1920s , everyone thought universe essentially static unchanging time .\n",
      "Then discovered universe expanding .\n",
      "Distant galaxy moving away u .\n",
      "This meant must closer together past .\n",
      "If extrapolate back , find must top 15 billion year ago .\n",
      "This Big Bang , beginning universe .\n",
      "But anything Big Bang ?\n",
      "If , created universe ?\n",
      "Why universe emerge Big Bang way ?\n",
      "We used think theory universe could divided two part .\n",
      "First , law like Maxwell ’ equation general relativity determined evolution universe , given state space one time .\n",
      "And second , question initial state universe .\n",
      "We made good progress first part , knowledge law evolution extreme condition .\n",
      "But recently , little idea initial condition universe .\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemmatized_sentences = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(i) for i in words if i not in stopwords]\n",
    "    lemmatized_sentences.append(\" \".join(words))\n",
    "    print(lemmatized_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dcf0cb",
   "metadata": {
    "papermill": {
     "duration": 0.004294,
     "end_time": "2023-07-02T04:57:09.831229",
     "exception": false,
     "start_time": "2023-07-02T04:57:09.826935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Comparing Stemming vs Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02a91016",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T04:57:09.842037Z",
     "iopub.status.busy": "2023-07-02T04:57:09.841638Z",
     "iopub.status.idle": "2023-07-02T04:57:09.847213Z",
     "shell.execute_reply": "2023-07-02T04:57:09.845945Z"
    },
    "papermill": {
     "duration": 0.013749,
     "end_time": "2023-07-02T04:57:09.849518",
     "exception": false,
     "start_time": "2023-07-02T04:57:09.835769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are alon univers ?\n",
      "Are alone universe ?\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_sentences[3])\n",
    "print(lemmatized_sentences[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27.510534,
   "end_time": "2023-07-02T04:57:12.828698",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-02T04:56:45.318164",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
