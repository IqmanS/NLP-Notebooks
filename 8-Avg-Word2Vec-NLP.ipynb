{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/iqmansingh/getting-started-with-nlp?scriptVersionId=135703733\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<img src=\"https://cdn.discordapp.com/attachments/1111599839663370271/1124998618529681428/NLP-Banner.jpg\">\n\n# **Getting Started with NLP**\n\n---\n\n### This notebook contains various types of Text Preprocessing Techniques used for NLP like\n 1. Tokenization\n 2. Stemming\n 3. Lemmatization\n 4. Vectorization\n  - sklearn.feature_extraction.text.CountVectorizer\n  - sklearn.feature_extraction.text.TfidfVectorizer\n  - Gensim.Word2Vec\n  - tf.keras.layers.Embedding","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport tensorflow as tf\nimport datetime\nimport warnings\nimport nltk\nimport random\nimport re\nimport sklearn\nimport zipfile\nimport gensim\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnltk.download('punkt',download_dir=\"/kaggle/working/\")\nnltk.download('wordnet',download_dir=\"/kaggle/working/\")\nnltk.download('stopwords',download_dir=\"/kaggle/working/\")\nnltk.data.path.append('/kaggle/working/')\n\nwith zipfile.ZipFile(\"/kaggle/working/corpora/wordnet.zip\", 'r') as zip_f:\n    zip_f.extractall(\"/kaggle/working/corpora/\")\n    \nwarnings.filterwarnings(\"ignore\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline\nplt.style.use('dark_background')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-04T12:31:07.200792Z","iopub.execute_input":"2023-07-04T12:31:07.201212Z","iopub.status.idle":"2023-07-04T12:31:07.568066Z","shell.execute_reply.started":"2023-07-04T12:31:07.201181Z","shell.execute_reply":"2023-07-04T12:31:07.566624Z"},"trusted":true},"execution_count":204,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /kaggle/working/...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /kaggle/working/...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /kaggle/working/...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/ted-ultimate-dataset/2020-05-01/ted_talks_en.csv\")\ndf.sort_values(by=\"views\",ascending=False,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:07.571047Z","iopub.execute_input":"2023-07-04T12:31:07.571576Z","iopub.status.idle":"2023-07-04T12:31:08.327476Z","shell.execute_reply.started":"2023-07-04T12:31:07.57151Z","shell.execute_reply":"2023-07-04T12:31:08.325491Z"},"trusted":true},"execution_count":205,"outputs":[]},{"cell_type":"code","source":"# Tim Urban: \"Inside the mind of a master procrastinator\" Speech                                            \nspeech = df.iloc[6].transcript\nspeech[0:100]","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.329692Z","iopub.execute_input":"2023-07-04T12:31:08.330461Z","iopub.status.idle":"2023-07-04T12:31:08.340561Z","shell.execute_reply.started":"2023-07-04T12:31:08.330411Z","shell.execute_reply":"2023-07-04T12:31:08.338822Z"},"trusted":true},"execution_count":206,"outputs":[{"execution_count":206,"output_type":"execute_result","data":{"text/plain":"'So in college, I was a government major, which means I had to write a lot of papers. Now, when a nor'"},"metadata":{}}]},{"cell_type":"markdown","source":"---\n# 1. Tokenization\n","metadata":{}},{"cell_type":"markdown","source":"# 1.1 Sentence Tokenization ","metadata":{}},{"cell_type":"code","source":"sentences = nltk.sent_tokenize(speech)\nsentences[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.344503Z","iopub.execute_input":"2023-07-04T12:31:08.3451Z","iopub.status.idle":"2023-07-04T12:31:08.368022Z","shell.execute_reply.started":"2023-07-04T12:31:08.345048Z","shell.execute_reply":"2023-07-04T12:31:08.366328Z"},"trusted":true},"execution_count":207,"outputs":[{"execution_count":207,"output_type":"execute_result","data":{"text/plain":"['So in college, I was a government major, which means I had to write a lot of papers.',\n 'Now, when a normal student writes a paper, they might spread the work out a little like this.',\n 'So, you know — (Laughter) you get started maybe a little slowly, but you get enough done in the first week that, with some heavier days later on, everything gets done, things stay civil.',\n '(Laughter) And I would want to do that like that.',\n 'That would be the plan.']"},"metadata":{}}]},{"cell_type":"markdown","source":"# 1.2 Word Tokenization ","metadata":{}},{"cell_type":"code","source":"words = nltk.word_tokenize(speech)\nlen(words)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.369897Z","iopub.execute_input":"2023-07-04T12:31:08.370435Z","iopub.status.idle":"2023-07-04T12:31:08.423812Z","shell.execute_reply.started":"2023-07-04T12:31:08.370396Z","shell.execute_reply":"2023-07-04T12:31:08.422467Z"},"trusted":true},"execution_count":208,"outputs":[{"execution_count":208,"output_type":"execute_result","data":{"text/plain":"2769"},"metadata":{}}]},{"cell_type":"code","source":"for i in range(random.randint(1,50),random.randint(100,200)):\n    print(words[i],end=\" \")","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.425911Z","iopub.execute_input":"2023-07-04T12:31:08.427075Z","iopub.status.idle":"2023-07-04T12:31:08.437272Z","shell.execute_reply.started":"2023-07-04T12:31:08.427032Z","shell.execute_reply":"2023-07-04T12:31:08.434996Z"},"trusted":true},"execution_count":209,"outputs":[{"name":"stdout","text":"when a normal student writes a paper , they might spread the work out a little like this . So , you know — ( Laughter ) you get started maybe a little slowly , but you get enough done in the first week that , with some heavier days later on , everything gets done , things stay civil . ( Laughter ) And I would want to do that like that . That would be the plan . I would have it all ready to go , but then , actually , the paper would come along , and then I would kind of do this . ( Laughter ) And that would happen every single paper . But then came my 90-page senior thesis , a paper you 're supposed to spend a year on . And I knew for a paper like that , my normal work flow was not an option . It was way too big a project ","output_type":"stream"}]},{"cell_type":"markdown","source":"---\n\n# 2. Stemming vs Lemmatization\n","metadata":{}},{"cell_type":"markdown","source":"# 2.1 Stemming","metadata":{}},{"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words(\"english\")\nprint(stopwords)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.440247Z","iopub.execute_input":"2023-07-04T12:31:08.440819Z","iopub.status.idle":"2023-07-04T12:31:08.455152Z","shell.execute_reply.started":"2023-07-04T12:31:08.440777Z","shell.execute_reply":"2023-07-04T12:31:08.453677Z"},"trusted":true},"execution_count":210,"outputs":[{"name":"stdout","text":"['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","output_type":"stream"}]},{"cell_type":"code","source":"stemmer = nltk.PorterStemmer()\nstemmedSentences = []\n\nfor j in range(len(sentences)):\n    words = nltk.word_tokenize(sentences[j])\n#     words = [re.sub('[!,*)@#%(&$_?.^]',\"\",i).lower() for i in words]\n    words = [re.sub(\"[^a-zA-Z0-9]\",\"\",i).lower().lstrip() for i in words]\n    words = [stemmer.stem(i) for i in words if i not in stopwords]\n    stemmedSentences.append(\" \".join(words))\nstemmedSentences[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.456923Z","iopub.execute_input":"2023-07-04T12:31:08.458278Z","iopub.status.idle":"2023-07-04T12:31:08.574492Z","shell.execute_reply.started":"2023-07-04T12:31:08.458227Z","shell.execute_reply":"2023-07-04T12:31:08.573149Z"},"trusted":true},"execution_count":211,"outputs":[{"execution_count":211,"output_type":"execute_result","data":{"text/plain":"['colleg  govern major  mean write lot paper ',\n ' normal student write paper  might spread work littl like ',\n ' know   laughter  get start mayb littl slowli  get enough done first week  heavier day later  everyth get done  thing stay civil ',\n ' laughter  would want like ',\n 'would plan ']"},"metadata":{}}]},{"cell_type":"markdown","source":"# 2.2 Lemmatization ","metadata":{}},{"cell_type":"code","source":"lemmatizer = nltk.stem.WordNetLemmatizer()\nlemmatizedSentences = []\n\nfor j in range(len(sentences)):\n    words = nltk.word_tokenize(sentences[j])\n#     words = [re.sub(\"[!,*)@#%(&$_?.:’'^]\",\"\",i).lower() for i in words]\n    words = [re.sub(\"[^a-zA-Z0-9]\",\"\",i).lower().strip() for i in words]\n    words = [lemmatizer.lemmatize(i) for i in words if i not in stopwords]\n    lemmatizedSentences.append(\" \".join(words))\nlemmatizedSentences[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.576246Z","iopub.execute_input":"2023-07-04T12:31:08.577548Z","iopub.status.idle":"2023-07-04T12:31:08.654808Z","shell.execute_reply.started":"2023-07-04T12:31:08.577477Z","shell.execute_reply":"2023-07-04T12:31:08.653375Z"},"trusted":true},"execution_count":212,"outputs":[{"execution_count":212,"output_type":"execute_result","data":{"text/plain":"['college  government major  mean write lot paper ',\n ' normal student writes paper  might spread work little like ',\n ' know   laughter  get started maybe little slowly  get enough done first week  heavier day later  everything get done  thing stay civil ',\n ' laughter  would want like ',\n 'would plan ']"},"metadata":{}}]},{"cell_type":"markdown","source":"# 2.3 Comparing Stemming vs Lemmatization","metadata":{}},{"cell_type":"code","source":"for i in range(5,10):\n    print(stemmedSentences[i])\n    print(lemmatizedSentences[i])\n    print(\"-\"*50)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.659579Z","iopub.execute_input":"2023-07-04T12:31:08.660035Z","iopub.status.idle":"2023-07-04T12:31:08.669737Z","shell.execute_reply.started":"2023-07-04T12:31:08.660003Z","shell.execute_reply":"2023-07-04T12:31:08.667446Z"},"trusted":true},"execution_count":213,"outputs":[{"name":"stdout","text":"would readi go   actual  paper would come along  would kind \nwould ready go   actually  paper would come along  would kind \n--------------------------------------------------\n laughter  would happen everi singl paper \n laughter  would happen every single paper \n--------------------------------------------------\ncame 90page senior thesi  paper suppos spend year \ncame 90page senior thesis  paper supposed spend year \n--------------------------------------------------\nknew paper like  normal work flow option \nknew paper like  normal work flow option \n--------------------------------------------------\nway big project \nway big project \n--------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"---\n\n# 3. Vectorization\n","metadata":{}},{"cell_type":"markdown","source":"# 3.1 Bag of Words (CountVectorizer)\n### - sklearn.feature_extraction.text.CountVectorizer","metadata":{}},{"cell_type":"code","source":"# Frequncy BoW\ncountVectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features=2000)\nX = countVectorizer.fit_transform(lemmatizedSentences).toarray() \nX.shape\n# 20 - no of sentences\n# 78 - no of features","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.671473Z","iopub.execute_input":"2023-07-04T12:31:08.671899Z","iopub.status.idle":"2023-07-04T12:31:08.689827Z","shell.execute_reply.started":"2023-07-04T12:31:08.671846Z","shell.execute_reply":"2023-07-04T12:31:08.688349Z"},"trusted":true},"execution_count":214,"outputs":[{"execution_count":214,"output_type":"execute_result","data":{"text/plain":"(142, 482)"},"metadata":{}}]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.69197Z","iopub.execute_input":"2023-07-04T12:31:08.693127Z","iopub.status.idle":"2023-07-04T12:31:08.702088Z","shell.execute_reply.started":"2023-07-04T12:31:08.693065Z","shell.execute_reply":"2023-07-04T12:31:08.700774Z"},"trusted":true},"execution_count":215,"outputs":[{"name":"stdout","text":"[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3.2 TF-IDF (Term Frequency * Inverse Document Frequency)\n### - sklearn.feature_extraction.text.TfidfVectorizer","metadata":{}},{"cell_type":"code","source":"tfidfVecorier = sklearn.feature_extraction.text.TfidfVectorizer(max_features=2000)\nX = tfidfVecorier.fit_transform(lemmatizedSentences).toarray() \nX.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.704246Z","iopub.execute_input":"2023-07-04T12:31:08.704972Z","iopub.status.idle":"2023-07-04T12:31:08.727597Z","shell.execute_reply.started":"2023-07-04T12:31:08.704929Z","shell.execute_reply":"2023-07-04T12:31:08.725793Z"},"trusted":true},"execution_count":216,"outputs":[{"execution_count":216,"output_type":"execute_result","data":{"text/plain":"(142, 482)"},"metadata":{}}]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.729356Z","iopub.execute_input":"2023-07-04T12:31:08.72979Z","iopub.status.idle":"2023-07-04T12:31:08.738997Z","shell.execute_reply.started":"2023-07-04T12:31:08.729757Z","shell.execute_reply":"2023-07-04T12:31:08.737583Z"},"trusted":true},"execution_count":217,"outputs":[{"name":"stdout","text":"[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3.3 Word2Vec","metadata":{}},{"cell_type":"code","source":"# speech1 = re.sub(\"[^a-zA-Z0-9\\s]\",\"\",speech).lower()\nwordVecSentences = []\n\nfor j in range(len(sentences)):\n    words = nltk.word_tokenize(re.sub(\"[^a-zA-Z0-9\\s]\",\"\",sentences[j]))\n    words = [i.lower().strip() for i in words if i not in stopwords]\n    wordVecSentences.append(words)\n[print(wordVecSentences[i]) for i in range(3)];","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:32:19.044785Z","iopub.execute_input":"2023-07-04T12:32:19.045328Z","iopub.status.idle":"2023-07-04T12:32:19.099235Z","shell.execute_reply.started":"2023-07-04T12:32:19.045293Z","shell.execute_reply":"2023-07-04T12:32:19.097694Z"},"trusted":true},"execution_count":239,"outputs":[{"name":"stdout","text":"['so', 'college', 'i', 'government', 'major', 'means', 'i', 'write', 'lot', 'papers']\n['now', 'normal', 'student', 'writes', 'paper', 'might', 'spread', 'work', 'little', 'like']\n['so', 'know', 'laughter', 'get', 'started', 'maybe', 'little', 'slowly', 'get', 'enough', 'done', 'first', 'week', 'heavier', 'days', 'later', 'everything', 'gets', 'done', 'things', 'stay', 'civil']\n","output_type":"stream"}]},{"cell_type":"code","source":"word2Vec = gensim.models.Word2Vec(wordVecSentences,min_count=1)\nprint(\"Length of Word2Vec Vocab:\",len(word2Vec.wv))","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.802911Z","iopub.execute_input":"2023-07-04T12:31:08.803945Z","iopub.status.idle":"2023-07-04T12:31:08.859252Z","shell.execute_reply.started":"2023-07-04T12:31:08.803907Z","shell.execute_reply":"2023-07-04T12:31:08.857635Z"},"trusted":true},"execution_count":219,"outputs":[{"name":"stdout","text":"Length of Word2Vec Vocab: 575\n","output_type":"stream"}]},{"cell_type":"code","source":"word2Vec.wv[\"good\"]","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.917871Z","iopub.execute_input":"2023-07-04T12:31:08.919755Z","iopub.status.idle":"2023-07-04T12:31:08.934923Z","shell.execute_reply.started":"2023-07-04T12:31:08.919458Z","shell.execute_reply":"2023-07-04T12:31:08.93355Z"},"trusted":true},"execution_count":223,"outputs":[{"execution_count":223,"output_type":"execute_result","data":{"text/plain":"array([-8.7385764e-03,  7.6911743e-03,  8.0137365e-03, -1.0687785e-03,\n        7.0834076e-03,  8.4049255e-03,  6.3676531e-03,  7.0108664e-03,\n        8.0570821e-03, -9.5732817e-03, -7.2213686e-03,  9.5317587e-03,\n       -5.6686443e-03, -2.2825995e-03, -3.3824297e-03,  4.5296713e-03,\n       -3.3953134e-03, -1.8759486e-03, -1.1732690e-03,  1.3023549e-03,\n        2.5288290e-03,  8.8041816e-03, -5.3585088e-03,  2.6844721e-03,\n       -9.7650746e-03, -7.9475138e-03,  2.5814411e-03,  6.6914572e-03,\n        8.8532967e-03,  5.7035079e-03, -6.2186299e-03, -8.9407517e-03,\n        3.0737192e-05, -7.7307997e-03, -7.9747047e-03, -4.4920738e-04,\n       -8.9009283e-03, -8.2602882e-04, -1.4954179e-03, -7.7554099e-03,\n       -2.2529138e-03, -4.9651666e-03, -4.2590206e-03,  7.6394891e-03,\n       -8.3245551e-03, -3.6238070e-04,  7.5763999e-03, -9.8938541e-03,\n        5.9386245e-03, -9.6813478e-03, -4.4315946e-03, -2.0397895e-03,\n       -6.7910412e-03,  2.8875035e-03,  8.5233096e-03,  7.3832534e-03,\n        2.6370620e-04,  3.8260887e-03, -7.0987223e-03,  3.6331369e-03,\n        8.8981343e-03, -2.7283113e-03,  3.5359953e-03, -6.7824121e-03,\n       -6.6544423e-03,  8.3891656e-03, -9.1254320e-03, -3.3352526e-03,\n        2.6264505e-03, -2.1446075e-03, -1.8704367e-03, -4.8887935e-03,\n       -5.2157398e-03, -4.0402063e-03,  2.3980076e-04, -2.2973288e-03,\n       -1.4814623e-03, -2.8098463e-03,  5.2465773e-03, -1.1128468e-03,\n        3.7464695e-03, -9.6396720e-03,  1.9413570e-03,  2.6648983e-03,\n        2.5648233e-03, -4.1263332e-03, -8.0704233e-03, -2.5492086e-04,\n       -1.3787556e-03,  8.6463597e-03,  5.7880199e-03, -6.5980935e-03,\n       -8.1564235e-03,  3.1569826e-03,  9.5051732e-03, -5.7271379e-03,\n        9.4565880e-03, -1.1599460e-03,  9.8843165e-03,  1.8296192e-03],\n      dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"word2Vec.wv.similar_by_word(\"good\")[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.873349Z","iopub.execute_input":"2023-07-04T12:31:08.874642Z","iopub.status.idle":"2023-07-04T12:31:08.891313Z","shell.execute_reply.started":"2023-07-04T12:31:08.874591Z","shell.execute_reply":"2023-07-04T12:31:08.889442Z"},"trusted":true},"execution_count":221,"outputs":[{"execution_count":221,"output_type":"execute_result","data":{"text/plain":"[('slowly', 0.32574841380119324),\n ('across', 0.30761581659317017),\n ('enough', 0.282128244638443),\n ('whatever', 0.25818419456481934),\n ('ready', 0.2548826336860657)]"},"metadata":{}}]},{"cell_type":"markdown","source":"# 3.4 AvgWord2Vec","metadata":{}},{"cell_type":"code","source":"word2Vec.wv.index_to_key[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.895438Z","iopub.execute_input":"2023-07-04T12:31:08.898055Z","iopub.status.idle":"2023-07-04T12:31:08.915013Z","shell.execute_reply.started":"2023-07-04T12:31:08.897999Z","shell.execute_reply":"2023-07-04T12:31:08.912809Z"},"trusted":true},"execution_count":222,"outputs":[{"execution_count":222,"output_type":"execute_result","data":{"text/plain":"['i', 'and', 'laughter', 'now', 'like']"},"metadata":{}}]},{"cell_type":"code","source":"print(\"so\" in word2Vec.wv.index_to_key)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:08.937232Z","iopub.execute_input":"2023-07-04T12:31:08.939Z","iopub.status.idle":"2023-07-04T12:31:08.947284Z","shell.execute_reply.started":"2023-07-04T12:31:08.938947Z","shell.execute_reply":"2023-07-04T12:31:08.946053Z"},"trusted":true},"execution_count":224,"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}]},{"cell_type":"code","source":"def avgWord2Vec(sent):\n    meanSent = [np.mean(word2Vec.wv[word]) for word in sent if (word in word2Vec.wv.index_to_key)]\n    return meanSent","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:36:29.75638Z","iopub.execute_input":"2023-07-04T12:36:29.756885Z","iopub.status.idle":"2023-07-04T12:36:29.764588Z","shell.execute_reply.started":"2023-07-04T12:36:29.756851Z","shell.execute_reply":"2023-07-04T12:36:29.762958Z"},"trusted":true},"execution_count":253,"outputs":[]},{"cell_type":"code","source":"avgWord2Vec([\"good\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:36:31.731714Z","iopub.execute_input":"2023-07-04T12:36:31.732609Z","iopub.status.idle":"2023-07-04T12:36:31.742217Z","shell.execute_reply.started":"2023-07-04T12:36:31.732551Z","shell.execute_reply":"2023-07-04T12:36:31.741059Z"},"trusted":true},"execution_count":254,"outputs":[{"execution_count":254,"output_type":"execute_result","data":{"text/plain":"[-0.00035779606]"},"metadata":{}}]},{"cell_type":"code","source":"avgW2VSentences = []\nfor i in range(len(wordVecSentences)):\n    avgW2VSentences.append(avgWord2Vec(wordVecSentences[i]))\n    \n[print(avgW2VSentences[i]) for i in range(1,10,5)];","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:38:06.209583Z","iopub.execute_input":"2023-07-04T12:38:06.210883Z","iopub.status.idle":"2023-07-04T12:38:06.251276Z","shell.execute_reply.started":"2023-07-04T12:38:06.210835Z","shell.execute_reply":"2023-07-04T12:38:06.249439Z"},"trusted":true},"execution_count":265,"outputs":[{"name":"stdout","text":"[0.00065491156, 8.164659e-05, 0.00026728242, 1.4213135e-05, 0.00066849, -0.00014551873, -0.0006034722, 0.00082181784, 0.00043954133, -0.00019839092]\n[-1.8426916e-05, 0.00067272916, 0.00013808568, -8.467185e-06, 0.0002796575, 0.00019812353, 0.00066849]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3.5 Word Embedding\n### - Keras OneHot removes Special Chars and Lowers the Text ","metadata":{}},{"cell_type":"code","source":"#One Hot Representation\nVOCAB_SIZE = 10000\n\noneHot = [tf.keras.preprocessing.text.one_hot(i,VOCAB_SIZE) for i in sentences]\nprint(oneHot[:2])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:09.028066Z","iopub.execute_input":"2023-07-04T12:31:09.028823Z","iopub.status.idle":"2023-07-04T12:31:09.04033Z","shell.execute_reply.started":"2023-07-04T12:31:09.028785Z","shell.execute_reply":"2023-07-04T12:31:09.038925Z"},"trusted":true},"execution_count":229,"outputs":[{"name":"stdout","text":"[[936, 336, 1359, 3489, 9436, 5133, 3473, 6153, 1483, 4018, 3489, 4249, 636, 8187, 5133, 1061, 4251, 5988], [4126, 2224, 5133, 7387, 8066, 6581, 5133, 8127, 9527, 2532, 8957, 2110, 7917, 3504, 5133, 1024, 1946, 8645]]\n","output_type":"stream"}]},{"cell_type":"code","source":"#Padding the Vectors\nMAXLEN = 50\n\npaddedVecs = tf.keras.utils.pad_sequences(oneHot,padding=\"pre\",maxlen=MAXLEN)\nprint(paddedVecs[:2])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:09.04194Z","iopub.execute_input":"2023-07-04T12:31:09.042778Z","iopub.status.idle":"2023-07-04T12:31:09.058178Z","shell.execute_reply.started":"2023-07-04T12:31:09.042739Z","shell.execute_reply":"2023-07-04T12:31:09.057164Z"},"trusted":true},"execution_count":230,"outputs":[{"name":"stdout","text":"[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0  936  336 1359 3489 9436 5133 3473 6153 1483 4018\n  3489 4249  636 8187 5133 1061 4251 5988]\n [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0 4126 2224 5133 7387 8066 6581 5133 8127 9527 2532\n  8957 2110 7917 3504 5133 1024 1946 8645]]\n","output_type":"stream"}]},{"cell_type":"code","source":"#Embedding Matrix\nDIMENSION = 100\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Embedding(VOCAB_SIZE,DIMENSION,input_length=MAXLEN))\nmodel.compile(optimizer=\"adam\",loss=\"mse\")\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:09.065026Z","iopub.execute_input":"2023-07-04T12:31:09.065503Z","iopub.status.idle":"2023-07-04T12:31:09.1244Z","shell.execute_reply.started":"2023-07-04T12:31:09.065469Z","shell.execute_reply":"2023-07-04T12:31:09.123394Z"},"trusted":true},"execution_count":231,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_1 (Embedding)     (None, 50, 100)           1000000   \n                                                                 \n=================================================================\nTotal params: 1,000,000\nTrainable params: 1,000,000\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"embeddedVecs = model.predict(paddedVecs)\nprint(embeddedVecs[0].shape)\n# 50 = no of input words\n# 100 = no of features in Embedding Matrix","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:31:09.125404Z","iopub.execute_input":"2023-07-04T12:31:09.125754Z","iopub.status.idle":"2023-07-04T12:31:09.26557Z","shell.execute_reply.started":"2023-07-04T12:31:09.125725Z","shell.execute_reply":"2023-07-04T12:31:09.264137Z"},"trusted":true},"execution_count":232,"outputs":[{"name":"stdout","text":"5/5 [==============================] - 0s 3ms/step\n(50, 100)\n","output_type":"stream"}]}]}