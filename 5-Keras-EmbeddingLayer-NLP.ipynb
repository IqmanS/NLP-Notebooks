{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/iqmansingh/getting-started-with-nlp?scriptVersionId=135564568\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<img src=\"https://cdn.discordapp.com/attachments/1111599839663370271/1124998618529681428/NLP-Banner.jpg\">\n\n# **Getting Started with NLP**\n\n---\n\n### This notebook contains various types of Text Preprocessing Techniques used for NLP like\n 1. Tokenization\n 2. Stemming\n 3. Lemmatization\n 4. Vectorization\n  - sklearn.feature_extraction.text.CountVectorizer\n  - sklearn.feature_extraction.text.TfidfVectorizer\n  - Gensim.Word2Vec\n  - tf.keras.layers.Embedding","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport tensorflow as tf\nimport datetime\nimport warnings\nimport nltk\nimport random\nimport re\nimport sklearn\nimport zipfile\nimport gensim\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnltk.download('punkt',download_dir=\"/kaggle/working/\")\nnltk.download('wordnet',download_dir=\"/kaggle/working/\")\nnltk.download('stopwords',download_dir=\"/kaggle/working/\")\nnltk.data.path.append('/kaggle/working/')\n\nwith zipfile.ZipFile(\"/kaggle/working/corpora/wordnet.zip\", 'r') as zip_f:\n    zip_f.extractall(\"/kaggle/working/corpora/\")\n    \nwarnings.filterwarnings(\"ignore\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline\nplt.style.use('dark_background')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-03T05:57:40.943615Z","iopub.execute_input":"2023-07-03T05:57:40.944616Z","iopub.status.idle":"2023-07-03T05:57:41.288973Z","shell.execute_reply.started":"2023-07-03T05:57:40.944565Z","shell.execute_reply":"2023-07-03T05:57:41.287695Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /kaggle/working/...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /kaggle/working/...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /kaggle/working/...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/ted-ultimate-dataset/2020-05-01/ted_talks_en.csv\")\ndf.sort_values(by=\"views\",ascending=False,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:41.291011Z","iopub.execute_input":"2023-07-03T05:57:41.291496Z","iopub.status.idle":"2023-07-03T05:57:42.037521Z","shell.execute_reply.started":"2023-07-03T05:57:41.291463Z","shell.execute_reply":"2023-07-03T05:57:42.036366Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# Tim Urban: \"Inside the mind of a master procrastinator\" Speech                                            \nspeech = df.iloc[6].transcript\nspeech","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:42.040253Z","iopub.execute_input":"2023-07-03T05:57:42.041318Z","iopub.status.idle":"2023-07-03T05:57:42.049715Z","shell.execute_reply.started":"2023-07-03T05:57:42.041271Z","shell.execute_reply":"2023-07-03T05:57:42.048647Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"'So in college, I was a government major, which means I had to write a lot of papers. Now, when a normal student writes a paper, they might spread the work out a little like this. So, you know — (Laughter) you get started maybe a little slowly, but you get enough done in the first week that, with some heavier days later on, everything gets done, things stay civil. (Laughter) And I would want to do that like that. That would be the plan. I would have it all ready to go, but then, actually, the paper would come along, and then I would kind of do this. (Laughter) And that would happen every single paper. But then came my 90-page senior thesis, a paper you\\'re supposed to spend a year on. And I knew for a paper like that, my normal work flow was not an option. It was way too big a project. So I planned things out, and I decided I kind of had to go something like this. This is how the year would go. So I\\'d start off light, and I\\'d bump it up in the middle months, and then at the end, I would kick it up into high gear just like a little staircase. How hard could it be to walk up the stairs? No big deal, right? But then, the funniest thing happened. Those first few months? They came and went, and I couldn\\'t quite do stuff. So we had an awesome new revised plan. (Laughter) And then — (Laughter) But then those middle months actually went by, and I didn\\'t really write words, and so we were here. And then two months turned into one month, which turned into two weeks. And one day I woke up with three days until the deadline, still not having written a word, and so I did the only thing I could: I wrote 90 pages over 72 hours, pulling not one but two all-nighters — humans are not supposed to pull two all-nighters — sprinted across campus, dove in slow motion, and got it in just at the deadline. I thought that was the end of everything. But a week later I get a call, and it\\'s the school. And they say, \"Is this Tim Urban?\" And I say, \"Yeah.\" And they say, \"We need to talk about your thesis.\" And I say, \"OK.\" And they say, \"It\\'s the best one we\\'ve ever seen.\" (Laughter) (Applause) That did not happen. (Laughter) It was a very, very bad thesis. (Laughter) I just wanted to enjoy that one moment when all of you thought, \"This guy is amazing!\" (Laughter) No, no, it was very, very bad. Anyway, today I\\'m a writer-blogger guy. I write the blog Wait But Why. And a couple of years ago, I decided to write about procrastination. My behavior has always perplexed the non-procrastinators around me, and I wanted to explain to the non-procrastinators of the world what goes on in the heads of procrastinators, and why we are the way we are. Now, I had a hypothesis that the brains of procrastinators were actually different than the brains of other people. And to test this, I found an MRI lab that actually let me scan both my brain and the brain of a proven non-procrastinator, so I could compare them. I actually brought them here to show you today. I want you to take a look carefully to see if you can notice a difference. I know that if you\\'re not a trained brain expert, it\\'s not that obvious, but just take a look, OK? So here\\'s the brain of a non-procrastinator. (Laughter) Now ... here\\'s my brain. (Laughter) There is a difference. Both brains have a Rational Decision-Maker in them, but the procrastinator\\'s brain also has an Instant Gratification Monkey. Now, what does this mean for the procrastinator? Well, it means everything\\'s fine until this happens. [This is a perfect time to get some work done.] [Nope!] So the Rational Decision-Maker will make the rational decision to do something productive, but the Monkey doesn\\'t like that plan, so he actually takes the wheel, and he says, \"Actually, let\\'s read the entire Wikipedia page of the Nancy Kerrigan/ Tonya Harding scandal, because I just remembered that that happened. (Laughter) Then — (Laughter) Then we\\'re going to go over to the fridge, to see if there\\'s anything new in there since 10 minutes ago. After that, we\\'re going to go on a YouTube spiral that starts with videos of Richard Feynman talking about magnets and ends much, much later with us watching interviews with Justin Bieber\\'s mom. (Laughter) \"All of that\\'s going to take a while, so we\\'re not going to really have room on the schedule for any work today. Sorry!\" (Sigh) Now, what is going on here? The Instant Gratification Monkey does not seem like a guy you want behind the wheel. He lives entirely in the present moment. He has no memory of the past, no knowledge of the future, and he only cares about two things: easy and fun. Now, in the animal world, that works fine. If you\\'re a dog and you spend your whole life doing nothing other than easy and fun things, you\\'re a huge success! (Laughter) And to the Monkey, humans are just another animal species. You have to keep well-slept, well-fed and propagating into the next generation, which in tribal times might have worked OK. But, if you haven\\'t noticed, now we\\'re not in tribal times. We\\'re in an advanced civilization, and the Monkey does not know what that is. Which is why we have another guy in our brain, the Rational Decision-Maker, who gives us the ability to do things no other animal can do. We can visualize the future. We can see the big picture. We can make long-term plans. And he wants to take all of that into account. And he wants to just have us do whatever makes sense to be doing right now. Now, sometimes it makes sense to be doing things that are easy and fun, like when you\\'re having dinner or going to bed or enjoying well-earned leisure time. That\\'s why there\\'s an overlap. Sometimes they agree. But other times, it makes much more sense to be doing things that are harder and less pleasant, for the sake of the big picture. And that\\'s when we have a conflict. And for the procrastinator, that conflict tends to end a certain way every time, leaving him spending a lot of time in this orange zone, an easy and fun place that\\'s entirely out of the Makes Sense circle. I call it the Dark Playground. (Laughter) Now, the Dark Playground is a place that all of you procrastinators out there know very well. It\\'s where leisure activities happen at times when leisure activities are not supposed to be happening. The fun you have in the Dark Playground isn\\'t actually fun, because it\\'s completely unearned, and the air is filled with guilt, dread, anxiety, self-hatred — all of those good procrastinator feelings. And the question is, in this situation, with the Monkey behind the wheel, how does the procrastinator ever get himself over here to this blue zone, a less pleasant place, but where really important things happen? Well, turns out the procrastinator has a guardian angel, someone who\\'s always looking down on him and watching over him in his darkest moments — someone called the Panic Monster. (Laughter) Now, the Panic Monster is dormant most of the time, but he suddenly wakes up anytime a deadline gets too close or there\\'s danger of public embarrassment, a career disaster or some other scary consequence. And importantly, he\\'s the only thing the Monkey is terrified of. Now, he became very relevant in my life pretty recently, because the people of TED reached out to me about six months ago and invited me to do a TED Talk. (Laughter) Now, of course, I said yes. It\\'s always been a dream of mine to have done a TED Talk in the past. (Laughter) (Applause) But in the middle of all this excitement, the Rational Decision-Maker seemed to have something else on his mind. He was saying, \"Are we clear on what we just accepted? Do we get what\\'s going to be now happening one day in the future? We need to sit down and work on this right now.\" And the Monkey said, \"Totally agree, but let\\'s just open Google Earth and zoom in to the bottom of India, like 200 feet above the ground, and scroll up for two and a half hours til we get to the top of the country, so we can get a better feel for India.\" (Laughter) So that\\'s what we did that day. (Laughter) As six months turned into four and then two and then one, the people of TED decided to release the speakers. And I opened up the website, and there was my face staring right back at me. And guess who woke up? (Laughter) So the Panic Monster starts losing his mind, and a few seconds later, the whole system\\'s in mayhem. (Laughter) And the Monkey — remember, he\\'s terrified of the Panic Monster — boom, he\\'s up the tree! And finally, finally, the Rational Decision-Maker can take the wheel and I can start working on the talk. Now, the Panic Monster explains all kinds of pretty insane procrastinator behavior, like how someone like me could spend two weeks unable to start the opening sentence of a paper, and then miraculously find the unbelievable work ethic to stay up all night and write eight pages. And this entire situation, with the three characters — this is the procrastinator\\'s system. It\\'s not pretty, but in the end, it works. This is what I decided to write about on the blog a couple of years ago. When I did, I was amazed by the response. Literally thousands of emails came in, from all different kinds of people from all over the world, doing all different kinds of things. These are people who were nurses, bankers, painters, engineers and lots and lots of PhD students. (Laughter) And they were all writing, saying the same thing: \"I have this problem too.\" But what struck me was the contrast between the light tone of the post and the heaviness of these emails. These people were writing with intense frustration about what procrastination had done to their lives, about what this Monkey had done to them. And I thought about this, and I said, well, if the procrastinator\\'s system works, then what\\'s going on? Why are all of these people in such a dark place? Well, it turns out that there\\'s two kinds of procrastination. Everything I\\'ve talked about today, the examples I\\'ve given, they all have deadlines. And when there\\'s deadlines, the effects of procrastination are contained to the short term because the Panic Monster gets involved. But there\\'s a second kind of procrastination that happens in situations when there is no deadline. So if you wanted a career where you\\'re a self-starter — something in the arts, something entrepreneurial — there\\'s no deadlines on those things at first, because nothing\\'s happening, not until you\\'ve gone out and done the hard work to get momentum, get things going. There\\'s also all kinds of important things outside of your career that don\\'t involve any deadlines, like seeing your family or exercising and taking care of your health, working on your relationship or getting out of a relationship that isn\\'t working. Now if the procrastinator\\'s only mechanism of doing these hard things is the Panic Monster, that\\'s a problem, because in all of these non-deadline situations, the Panic Monster doesn\\'t show up. He has nothing to wake up for, so the effects of procrastination, they\\'re not contained; they just extend outward forever. And it\\'s this long-term kind of procrastination that\\'s much less visible and much less talked about than the funnier, short-term deadline-based kind. It\\'s usually suffered quietly and privately. And it can be the source of a huge amount of long-term unhappiness, and regrets. And I thought, that\\'s why those people are emailing, and that\\'s why they\\'re in such a bad place. It\\'s not that they\\'re cramming for some project. It\\'s that long-term procrastination has made them feel like a spectator, at times, in their own lives. The frustration is not that they couldn\\'t achieve their dreams; it\\'s that they weren\\'t even able to start chasing them. So I read these emails and I had a little bit of an epiphany — that I don\\'t think non-procrastinators exist. That\\'s right — I think all of you are procrastinators. Now, you might not all be a mess, like some of us, (Laughter) and some of you may have a healthy relationship with deadlines, but remember: the Monkey\\'s sneakiest trick is when the deadlines aren\\'t there. Now, I want to show you one last thing. I call this a Life Calendar. That\\'s one box for every week of a 90-year life. That\\'s not that many boxes, especially since we\\'ve already used a bunch of those. So I think we need to all take a long, hard look at that calendar. We need to think about what we\\'re really procrastinating on, because everyone is procrastinating on something in life. We need to stay aware of the Instant Gratification Monkey. That\\'s a job for all of us. And because there\\'s not that many boxes on there, it\\'s a job that should probably start today. Well, maybe not today, but ... (Laughter) You know. Sometime soon. Thank you. (Applause)'"},"metadata":{}}]},{"cell_type":"markdown","source":"---\n\n# 1. Tokenization\n### 1.1 Sentence Tokenization ","metadata":{}},{"cell_type":"code","source":"sentences = nltk.sent_tokenize(speech)\nsentences[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:42.052559Z","iopub.execute_input":"2023-07-03T05:57:42.052885Z","iopub.status.idle":"2023-07-03T05:57:42.072832Z","shell.execute_reply.started":"2023-07-03T05:57:42.052857Z","shell.execute_reply":"2023-07-03T05:57:42.071483Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"['So in college, I was a government major, which means I had to write a lot of papers.',\n 'Now, when a normal student writes a paper, they might spread the work out a little like this.',\n 'So, you know — (Laughter) you get started maybe a little slowly, but you get enough done in the first week that, with some heavier days later on, everything gets done, things stay civil.',\n '(Laughter) And I would want to do that like that.',\n 'That would be the plan.']"},"metadata":{}}]},{"cell_type":"markdown","source":"### 1.2 Word Tokenization ","metadata":{}},{"cell_type":"code","source":"words = nltk.word_tokenize(speech)\nlen(words)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:42.074124Z","iopub.execute_input":"2023-07-03T05:57:42.074819Z","iopub.status.idle":"2023-07-03T05:57:42.122437Z","shell.execute_reply.started":"2023-07-03T05:57:42.074784Z","shell.execute_reply":"2023-07-03T05:57:42.121292Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"2769"},"metadata":{}}]},{"cell_type":"code","source":"for i in range(random.randint(1,50),random.randint(100,200)):\n    print(words[i],end=\" \")","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:42.124196Z","iopub.execute_input":"2023-07-03T05:57:42.124542Z","iopub.status.idle":"2023-07-03T05:57:42.132377Z","shell.execute_reply.started":"2023-07-03T05:57:42.124513Z","shell.execute_reply":"2023-07-03T05:57:42.131122Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"they might spread the work out a little like this . So , you know — ( Laughter ) you get started maybe a little slowly , but you get enough done in the first week that , with some heavier days later on , everything gets done , things stay civil . ( Laughter ) And I would want to do that like that . That would be the plan . I would have it all ready to go , but then , actually , the paper would come along , and then I would kind of do this . ( Laughter ) And that would happen every single paper . But then came my 90-page senior thesis , a paper you 're supposed to spend a year on . And I knew for a paper like that , my normal work flow was not an option . It was way too big a project . So I planned things out , and I decided I kind ","output_type":"stream"}]},{"cell_type":"markdown","source":"---\n\n# 2. Stemming vs Lemmatization\n\n### 2.1 Stemming","metadata":{}},{"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words(\"english\")\nprint(stopwords)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:42.133682Z","iopub.execute_input":"2023-07-03T05:57:42.133993Z","iopub.status.idle":"2023-07-03T05:57:42.14585Z","shell.execute_reply.started":"2023-07-03T05:57:42.133966Z","shell.execute_reply":"2023-07-03T05:57:42.144447Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","output_type":"stream"}]},{"cell_type":"code","source":"stemmer = nltk.PorterStemmer()\nstemmedSentences = []\n\nfor j in range(len(sentences)):\n    words = nltk.word_tokenize(sentences[j])\n#     words = [re.sub('[!,*)@#%(&$_?.^]',\"\",i).lower() for i in words]\n    words = [re.sub(\"[^a-zA-Z0-9]\",\"\",i).lower().lstrip() for i in words]\n    words = [stemmer.stem(i) for i in words if i not in stopwords]\n    stemmedSentences.append(\" \".join(words))\nstemmedSentences[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:42.14723Z","iopub.execute_input":"2023-07-03T05:57:42.147558Z","iopub.status.idle":"2023-07-03T05:57:42.254209Z","shell.execute_reply.started":"2023-07-03T05:57:42.147531Z","shell.execute_reply":"2023-07-03T05:57:42.253003Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"['colleg  govern major  mean write lot paper ',\n ' normal student write paper  might spread work littl like ',\n ' know   laughter  get start mayb littl slowli  get enough done first week  heavier day later  everyth get done  thing stay civil ',\n ' laughter  would want like ',\n 'would plan ']"},"metadata":{}}]},{"cell_type":"markdown","source":"### 2.2 Lemmatization ","metadata":{}},{"cell_type":"code","source":"lemmatizer = nltk.stem.WordNetLemmatizer()\nlemmatizedSentences = []\n\nfor j in range(len(sentences)):\n    words = nltk.word_tokenize(sentences[j])\n#     words = [re.sub(\"[!,*)@#%(&$_?.:’'^]\",\"\",i).lower() for i in words]\n    words = [re.sub(\"[^a-zA-Z0-9]\",\"\",i).lower().strip() for i in words]\n    words = [lemmatizer.lemmatize(i) for i in words if i not in stopwords]\n    lemmatizedSentences.append(\" \".join(words))\nlemmatizedSentences[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:42.255577Z","iopub.execute_input":"2023-07-03T05:57:42.255926Z","iopub.status.idle":"2023-07-03T05:57:42.330049Z","shell.execute_reply.started":"2023-07-03T05:57:42.255896Z","shell.execute_reply":"2023-07-03T05:57:42.329094Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"['college  government major  mean write lot paper ',\n ' normal student writes paper  might spread work little like ',\n ' know   laughter  get started maybe little slowly  get enough done first week  heavier day later  everything get done  thing stay civil ',\n ' laughter  would want like ',\n 'would plan ']"},"metadata":{}}]},{"cell_type":"markdown","source":"### 2.3 Comparing Stemming vs Lemmatization","metadata":{}},{"cell_type":"code","source":"print(stemmedSentences[6])\nprint(lemmatizedSentences[6])","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:42.333543Z","iopub.execute_input":"2023-07-03T05:57:42.334257Z","iopub.status.idle":"2023-07-03T05:57:42.339109Z","shell.execute_reply.started":"2023-07-03T05:57:42.334209Z","shell.execute_reply":"2023-07-03T05:57:42.338336Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":" laughter  would happen everi singl paper \n laughter  would happen every single paper \n","output_type":"stream"}]},{"cell_type":"markdown","source":"---\n\n# 3. Vectorization\n### 3.1 Bag of Words (CountVectorizer)\n- sklearn.feature_extraction.text.CountVectorizer","metadata":{}},{"cell_type":"code","source":"# Frequncy BoW\ncountVectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features=2000)\nX = countVectorizer.fit_transform(lemmatizedSentences).toarray() \nX.shape\n# 20 - no of sentences\n# 78 - no of features","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:42.340584Z","iopub.execute_input":"2023-07-03T05:57:42.341194Z","iopub.status.idle":"2023-07-03T05:57:42.358401Z","shell.execute_reply.started":"2023-07-03T05:57:42.341162Z","shell.execute_reply":"2023-07-03T05:57:42.35729Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"(142, 482)"},"metadata":{}}]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:42.360005Z","iopub.execute_input":"2023-07-03T05:57:42.360373Z","iopub.status.idle":"2023-07-03T05:57:42.37302Z","shell.execute_reply.started":"2023-07-03T05:57:42.360314Z","shell.execute_reply":"2023-07-03T05:57:42.371663Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 3.2 TF-IDF (Term Frequency * Inverse Document Frequency)\n - sklearn.feature_extraction.text.TfidfVectorizer","metadata":{}},{"cell_type":"code","source":"tfidfVecorier = sklearn.feature_extraction.text.TfidfVectorizer(max_features=2000)\nX = tfidfVecorier.fit_transform(lemmatizedSentences).toarray() \nX.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:42.374617Z","iopub.execute_input":"2023-07-03T05:57:42.37516Z","iopub.status.idle":"2023-07-03T05:57:42.391116Z","shell.execute_reply.started":"2023-07-03T05:57:42.375115Z","shell.execute_reply":"2023-07-03T05:57:42.390225Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"(142, 482)"},"metadata":{}}]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:42.392519Z","iopub.execute_input":"2023-07-03T05:57:42.393084Z","iopub.status.idle":"2023-07-03T05:57:42.402489Z","shell.execute_reply.started":"2023-07-03T05:57:42.393054Z","shell.execute_reply":"2023-07-03T05:57:42.401464Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 3.3 Word2Vec","metadata":{}},{"cell_type":"code","source":"# speech1 = re.sub(\"[^a-zA-Z0-9\\s]\",\"\",speech).lower()\nwordVecSentences = []\n\nfor j in range(len(sentences)):\n    words = nltk.word_tokenize(re.sub(\"[^a-zA-Z0-9\\s]\",\"\",speech))\n    words = [i for i in words if i not in stopwords]\n    wordVecSentences.append(words)\nprint(wordVecSentences[0][50:60])","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:42.403628Z","iopub.execute_input":"2023-07-03T05:57:42.403967Z","iopub.status.idle":"2023-07-03T05:57:45.702899Z","shell.execute_reply.started":"2023-07-03T05:57:42.403938Z","shell.execute_reply":"2023-07-03T05:57:45.701697Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"['plan', 'I', 'would', 'ready', 'go', 'actually', 'paper', 'would', 'come', 'along']\n","output_type":"stream"}]},{"cell_type":"code","source":"word2Vec = gensim.models.Word2Vec(wordVecSentences,min_count=1)\nprint(\"Length of Word2Vec Vocab:\",len(word2Vec.wv),\"\\n\")\nword2Vec.wv.most_similar(positive=[\"actually\"])[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T05:57:45.704456Z","iopub.execute_input":"2023-07-03T05:57:45.704983Z","iopub.status.idle":"2023-07-03T05:57:46.524486Z","shell.execute_reply.started":"2023-07-03T05:57:45.704943Z","shell.execute_reply":"2023-07-03T05:57:46.52299Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"Length of Word2Vec Vocab: 588 \n\n","output_type":"stream"},{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"[('plan', 0.7365055084228516),\n ('lab', 0.734563410282135),\n ('let', 0.7147486209869385),\n ('scan', 0.7056334614753723),\n ('takes', 0.6901529431343079)]"},"metadata":{}}]},{"cell_type":"markdown","source":"### 3.4 Word Embedding\n- Keras OneHot removes Special Chars and Lowers the Text ","metadata":{}},{"cell_type":"code","source":"#One Hot Representation\nVOCAB_SIZE = 10000\n\noneHot = [tf.keras.preprocessing.text.one_hot(i,VOCAB_SIZE) for i in sentences]\nprint(oneHot[:2])","metadata":{"execution":{"iopub.status.busy":"2023-07-03T06:00:36.73689Z","iopub.execute_input":"2023-07-03T06:00:36.737343Z","iopub.status.idle":"2023-07-03T06:00:36.74808Z","shell.execute_reply.started":"2023-07-03T06:00:36.73731Z","shell.execute_reply":"2023-07-03T06:00:36.746621Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"[[630, 1988, 8608, 1478, 6902, 2303, 5394, 1619, 1853, 7177, 1478, 6432, 9512, 1035, 2303, 5146, 6685, 1582], [3799, 8962, 2303, 6123, 8366, 1713, 2303, 1434, 4228, 7053, 8429, 6763, 3452, 1493, 2303, 218, 8593, 1898]]\n","output_type":"stream"}]},{"cell_type":"code","source":"#Padding the Vectors\nMAXLEN = 50\n\npaddedVecs = tf.keras.utils.pad_sequences(oneHot,padding=\"pre\",maxlen=MAXLEN)\nprint(paddedVecs[:2])","metadata":{"execution":{"iopub.status.busy":"2023-07-03T06:08:38.639863Z","iopub.execute_input":"2023-07-03T06:08:38.640427Z","iopub.status.idle":"2023-07-03T06:08:38.650465Z","shell.execute_reply.started":"2023-07-03T06:08:38.640381Z","shell.execute_reply":"2023-07-03T06:08:38.649459Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stdout","text":"[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0  630 1988 8608 1478 6902 2303 5394 1619 1853 7177\n  1478 6432 9512 1035 2303 5146 6685 1582]\n [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0 3799 8962 2303 6123 8366 1713 2303 1434 4228 7053\n  8429 6763 3452 1493 2303  218 8593 1898]]\n","output_type":"stream"}]},{"cell_type":"code","source":"#Embedding Matrix\nDIMENSION = 100\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Embedding(VOCAB_SIZE,DIMENSION,input_length=MAXLEN))\nmodel.compile(optimizer=\"adam\",loss=\"mse\")\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T06:10:20.345066Z","iopub.execute_input":"2023-07-03T06:10:20.345512Z","iopub.status.idle":"2023-07-03T06:10:20.396769Z","shell.execute_reply.started":"2023-07-03T06:10:20.345478Z","shell.execute_reply":"2023-07-03T06:10:20.395627Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_2 (Embedding)     (None, 50, 100)           1000000   \n                                                                 \n=================================================================\nTotal params: 1,000,000\nTrainable params: 1,000,000\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"embeddedVecs = model.predict(paddedVecs)\nprint(embeddedVecs[0].shape)\n# 50 = no of input words\n# 100 = no of features in Embedding Matrix","metadata":{"execution":{"iopub.status.busy":"2023-07-03T06:11:49.910244Z","iopub.execute_input":"2023-07-03T06:11:49.910671Z","iopub.status.idle":"2023-07-03T06:11:50.00168Z","shell.execute_reply.started":"2023-07-03T06:11:49.910631Z","shell.execute_reply":"2023-07-03T06:11:50.000727Z"},"trusted":true},"execution_count":118,"outputs":[{"name":"stdout","text":"5/5 [==============================] - 0s 2ms/step\n(50, 100)\n","output_type":"stream"}]}]}